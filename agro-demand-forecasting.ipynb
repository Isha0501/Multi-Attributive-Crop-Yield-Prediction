{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7247975,"sourceType":"datasetVersion","datasetId":4199008},{"sourceId":7815897,"sourceType":"datasetVersion","datasetId":4578757},{"sourceId":8666227,"sourceType":"datasetVersion","datasetId":5193159}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-04-30T14:26:33.099822Z","iopub.execute_input":"2025-04-30T14:26:33.100245Z","iopub.status.idle":"2025-04-30T14:26:33.649853Z","shell.execute_reply.started":"2025-04-30T14:26:33.100195Z","shell.execute_reply":"2025-04-30T14:26:33.648485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#this dataset is giving us the conditions required to grow a particular crop\n#no info on where these conditions are found\n# Crop_Production_Statistics does not have rainfall, thus using crop_yield\n\nsoil=pd.read_csv('/kaggle/input/crop-recommendation/Crop_recommendation.csv')\ncrops=pd.read_csv('/kaggle/input/crop-yield/crop_yield.csv')\n# crops=pd.read_csv('/kaggle/input/india-crop-yield/Crop_Production_Statistics.csv')","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:26:33.651431Z","iopub.execute_input":"2025-04-30T14:26:33.651896Z","iopub.status.idle":"2025-04-30T14:26:33.751350Z","shell.execute_reply.started":"2025-04-30T14:26:33.651850Z","shell.execute_reply":"2025-04-30T14:26:33.749914Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"soil.head()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2025-04-30T14:26:47.519301Z","iopub.execute_input":"2025-04-30T14:26:47.520518Z","iopub.status.idle":"2025-04-30T14:26:47.541991Z","shell.execute_reply.started":"2025-04-30T14:26:47.520477Z","shell.execute_reply":"2025-04-30T14:26:47.540510Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"crops.head()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:27:54.906842Z","iopub.execute_input":"2025-04-30T14:27:54.907442Z","iopub.status.idle":"2025-04-30T14:27:54.931059Z","shell.execute_reply.started":"2025-04-30T14:27:54.907398Z","shell.execute_reply":"2025-04-30T14:27:54.929703Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"soil['rainfall']=np.ceil(soil['rainfall'])","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:28:40.104942Z","iopub.execute_input":"2025-04-30T14:28:40.105349Z","iopub.status.idle":"2025-04-30T14:28:40.113071Z","shell.execute_reply.started":"2025-04-30T14:28:40.105318Z","shell.execute_reply":"2025-04-30T14:28:40.111864Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"crops['State'].nunique()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:28:46.051913Z","iopub.execute_input":"2025-04-30T14:28:46.052275Z","iopub.status.idle":"2025-04-30T14:28:46.065012Z","shell.execute_reply.started":"2025-04-30T14:28:46.052242Z","shell.execute_reply":"2025-04-30T14:28:46.063599Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"crops.State.unique()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:28:49.135206Z","iopub.execute_input":"2025-04-30T14:28:49.136403Z","iopub.status.idle":"2025-04-30T14:28:49.144198Z","shell.execute_reply.started":"2025-04-30T14:28:49.136331Z","shell.execute_reply":"2025-04-30T14:28:49.142959Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"crops.Crop.unique()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:28:52.949736Z","iopub.execute_input":"2025-04-30T14:28:52.950117Z","iopub.status.idle":"2025-04-30T14:28:52.959019Z","shell.execute_reply.started":"2025-04-30T14:28:52.950088Z","shell.execute_reply":"2025-04-30T14:28:52.957867Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"crops['mean_rainfall']=np.ceil(crops['Annual_Rainfall']/12)\ncrops.head()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:28:56.683636Z","iopub.execute_input":"2025-04-30T14:28:56.684004Z","iopub.status.idle":"2025-04-30T14:28:56.703008Z","shell.execute_reply.started":"2025-04-30T14:28:56.683979Z","shell.execute_reply":"2025-04-30T14:28:56.701859Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame(soil)\n\n# Filter the dataframe\nsoil_filtered_df = df[(df['label']=='maize')]\n\n# Print the filtered dataframe\nprint(soil_filtered_df)","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:29:22.697620Z","iopub.execute_input":"2025-04-30T14:29:22.697987Z","iopub.status.idle":"2025-04-30T14:29:22.752630Z","shell.execute_reply.started":"2025-04-30T14:29:22.697961Z","shell.execute_reply":"2025-04-30T14:29:22.751243Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"soil.label.unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T14:30:00.044801Z","iopub.execute_input":"2025-04-30T14:30:00.045189Z","iopub.status.idle":"2025-04-30T14:30:00.055290Z","shell.execute_reply.started":"2025-04-30T14:30:00.045160Z","shell.execute_reply":"2025-04-30T14:30:00.053312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame(crops)\n\n# Filter the dataframe\ncrop_filtered_df = df[(df['Crop']=='Maize')]\n\n# Print the filtered dataframe\nprint(crop_filtered_df)","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:31:11.708508Z","iopub.execute_input":"2025-04-30T14:31:11.708881Z","iopub.status.idle":"2025-04-30T14:31:11.725405Z","shell.execute_reply.started":"2025-04-30T14:31:11.708855Z","shell.execute_reply":"2025-04-30T14:31:11.724122Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df = pd.merge(soil_filtered_df, crop_filtered_df, left_on=['rainfall'], right_on=['mean_rainfall'])\nprint(merged_df)","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:31:32.160093Z","iopub.execute_input":"2025-04-30T14:31:32.160595Z","iopub.status.idle":"2025-04-30T14:31:32.190813Z","shell.execute_reply.started":"2025-04-30T14:31:32.160561Z","shell.execute_reply":"2025-04-30T14:31:32.189733Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df.Crop_Year.unique()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:31:36.681784Z","iopub.execute_input":"2025-04-30T14:31:36.682158Z","iopub.status.idle":"2025-04-30T14:31:36.689902Z","shell.execute_reply.started":"2025-04-30T14:31:36.682130Z","shell.execute_reply":"2025-04-30T14:31:36.688683Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df=merged_df.drop(columns= ['label', 'Crop','mean_rainfall'])","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:31:51.550792Z","iopub.execute_input":"2025-04-30T14:31:51.551145Z","iopub.status.idle":"2025-04-30T14:31:51.559534Z","shell.execute_reply.started":"2025-04-30T14:31:51.551118Z","shell.execute_reply":"2025-04-30T14:31:51.558163Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df.to_csv('merged_df.csv',index=False)\nmerged_df.head()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:32:03.777778Z","iopub.execute_input":"2025-04-30T14:32:03.778156Z","iopub.status.idle":"2025-04-30T14:32:03.811841Z","shell.execute_reply.started":"2025-04-30T14:32:03.778125Z","shell.execute_reply":"2025-04-30T14:32:03.810644Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df.info()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:32:14.907794Z","iopub.execute_input":"2025-04-30T14:32:14.908194Z","iopub.status.idle":"2025-04-30T14:32:14.926497Z","shell.execute_reply.started":"2025-04-30T14:32:14.908164Z","shell.execute_reply":"2025-04-30T14:32:14.925031Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df.sample(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T14:38:54.474109Z","iopub.execute_input":"2025-04-30T14:38:54.474531Z","iopub.status.idle":"2025-04-30T14:38:54.493932Z","shell.execute_reply.started":"2025-04-30T14:38:54.474498Z","shell.execute_reply":"2025-04-30T14:38:54.492601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pandas seaborn matplotlib","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LABEL ENCODING NON-NUMERIC COLUMNS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Assuming merged_df is your DataFrame\n# Identify non-numeric columns\nnon_numeric_columns = merged_df.select_dtypes(include=['object']).columns\n\n# Initialize the label encoder\nlabel_encoders = {}\nfor column in non_numeric_columns:\n    label_encoders[column] = LabelEncoder()\n    merged_df[column] = label_encoders[column].fit_transform(merged_df[column])\n\n# Display the first few rows of the updated DataFrame\nprint(merged_df.head(15))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:19:28.129952Z","iopub.execute_input":"2024-10-08T06:19:28.130389Z","iopub.status.idle":"2024-10-08T06:19:28.695891Z","shell.execute_reply.started":"2024-10-08T06:19:28.130348Z","shell.execute_reply":"2024-10-08T06:19:28.694566Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df_1=merged_df.drop(columns=['Production'])\nmerged_df_1.to_csv('merged_df_1.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:19:28.697213Z","iopub.execute_input":"2024-10-08T06:19:28.697612Z","iopub.status.idle":"2024-10-08T06:19:28.723934Z","shell.execute_reply.started":"2024-10-08T06:19:28.697578Z","shell.execute_reply":"2024-10-08T06:19:28.722703Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Assuming merged_df is your DataFrame\n# Identify non-numeric columns\nnon_numeric_columns = merged_df_1.select_dtypes(include=['object']).columns\n\n# Initialize the label encoder\nlabel_encoders = {}\nfor column in non_numeric_columns:\n    label_encoders[column] = LabelEncoder()\n    merged_df_1[column] = label_encoders[column].fit_transform(merged_df_1[column])\n\n# Display the first few rows of the updated DataFrame\nprint(merged_df_1.head(15))","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:19:46.263252Z","iopub.execute_input":"2024-10-08T06:19:46.263696Z","iopub.status.idle":"2024-10-08T06:19:46.283980Z","shell.execute_reply.started":"2024-10-08T06:19:46.263662Z","shell.execute_reply":"2024-10-08T06:19:46.282544Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GENERATING HEATMAP","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(42,42))\nsns.heatmap(merged_df.corr(), annot=True)\nplt.savefig('/kaggle/working/correlation_heatmap.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:19:49.254357Z","iopub.execute_input":"2024-10-08T06:19:49.254837Z","iopub.status.idle":"2024-10-08T06:19:53.214141Z","shell.execute_reply.started":"2024-10-08T06:19:49.254800Z","shell.execute_reply":"2024-10-08T06:19:53.212879Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(42,42))\nsns.heatmap(merged_df_1.corr(), annot=True)\nplt.savefig('/kaggle/working/correlation_heatmap_wo_prod.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:19:53.216109Z","iopub.execute_input":"2024-10-08T06:19:53.216497Z","iopub.status.idle":"2024-10-08T06:19:56.574989Z","shell.execute_reply.started":"2024-10-08T06:19:53.216465Z","shell.execute_reply":"2024-10-08T06:19:56.573751Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RANDOM FOREST","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# Display the first few rows of the dataset\nprint(data.head())\n\n# Separate features and target variable\nX = data.drop(columns=['Yield'])\ny = data['Yield']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year','Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the training and testing data\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:19:59.471689Z","iopub.execute_input":"2024-10-08T06:19:59.472784Z","iopub.status.idle":"2024-10-08T06:19:59.853990Z","shell.execute_reply.started":"2024-10-08T06:19:59.472727Z","shell.execute_reply":"2024-10-08T06:19:59.852649Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Initialize the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:20:01.045395Z","iopub.execute_input":"2024-10-08T06:20:01.046276Z","iopub.status.idle":"2024-10-08T06:20:02.795006Z","shell.execute_reply.started":"2024-10-08T06:20:01.046236Z","shell.execute_reply":"2024-10-08T06:20:02.793815Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PROPHET","metadata":{}},{"cell_type":"code","source":"# pip install prophet","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:08.686351Z","iopub.execute_input":"2024-09-09T14:43:08.686656Z","iopub.status.idle":"2024-09-09T14:43:08.691112Z","shell.execute_reply.started":"2024-09-09T14:43:08.686632Z","shell.execute_reply":"2024-09-09T14:43:08.690113Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# from prophet import Prophet\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# import numpy as np\n\n# # Load the dataset\n# data = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# # Drop the 'Crop' column since it has only one type of crop\n# # data = data.drop(columns=['Crop'])\n\n# # Strip whitespace from 'Season' values\n# data['Season'] = data['Season'].str.strip()\n\n# # Convert 'Crop_Year' and 'Season' into a datetime format\n# # Map seasons to months (assuming a mapping)\n# season_to_month = {\n#     'Spring': '03-01',\n#     'Summer': '06-01',\n#     'Autumn': '09-01',\n#     'Winter': '12-01',\n#     'Kharif': '07-01',  # Example for Kharif season\n#     'Rabi': '01-01'    # Example for Rabi season\n# }\n\n# # Handle missing or unexpected seasons\n# def get_season_month(season):\n#     return season_to_month.get(season, '01-01')\n\n# data['Date'] = data.apply(lambda row: f\"{row['Crop_Year']}-{get_season_month(row['Season'])}\", axis=1)\n# data['Date'] = pd.to_datetime(data['Date'])\n\n# # Prepare the dataframe for Prophet\n# df = data.rename(columns={'Date': 'ds', 'Yield': 'y'})\n# df = df[['ds', 'y', 'Annual_Rainfall', 'Fertilizer', 'Pesticide', 'rainfall']]\n\n# # Initialize the Prophet model\n# model = Prophet()\n\n# # Add additional regressors\n# model.add_regressor('Annual_Rainfall')\n# model.add_regressor('Fertilizer')\n# model.add_regressor('Pesticide')\n# # model.add_regressor('Production')\n# model.add_regressor('rainfall')\n\n# # Fit the model\n# model.fit(df)\n\n# # Make future dataframe for predictions\n# future = model.make_future_dataframe(periods=12, freq='M')\n# # Fill the future dataframe with the mean of the regressors for now\n# future['Annual_Rainfall'] = df['Annual_Rainfall'].mean()\n# future['Fertilizer'] = df['Fertilizer'].mean()\n# future['Pesticide'] = df['Pesticide'].mean()\n# # future['Production'] = df['Production'].mean()\n# future['rainfall'] = df['rainfall'].mean()\n\n# # Make predictions\n# forecast = model.predict(future)\n\n# # Evaluate the model on the historical data\n# df_forecast = forecast[['ds', 'yhat']].merge(df, on='ds')\n# mae = mean_absolute_error(df_forecast['y'], df_forecast['yhat'])\n# mse = mean_squared_error(df_forecast['y'], df_forecast['yhat'])\n# rmse = np.sqrt(mse)\n# r2 = r2_score(df_forecast['y'], df_forecast['yhat'])\n\n# print(f'MAE: {mae}')\n# print(f'MSE: {mse}')\n# print(f'RMSE: {rmse}')\n# print(f'R2: {r2}')\n\n# # Plot the forecast\n# model.plot(forecast)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:08.692392Z","iopub.execute_input":"2024-09-09T14:43:08.692716Z","iopub.status.idle":"2024-09-09T14:43:08.702529Z","shell.execute_reply.started":"2024-09-09T14:43:08.692690Z","shell.execute_reply":"2024-09-09T14:43:08.701492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Back Propagation Neural Network (BPNN)","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense\n# import numpy as np\n\n# # Load the dataset\n# data = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# # Drop the 'Crop' column since it has only one type of crop\n# # data = data.drop(columns=['Crop'])\n\n# # Separate features and target variable\n# X = data.drop(columns=['Yield'])\n# y = data['Yield']\n\n# # Identify categorical and numerical columns\n# categorical_cols = ['Crop_Year', 'Season', 'State']\n# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# # Preprocessing for numerical data: impute missing values and scale features\n# numerical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean')),\n#     ('scaler', StandardScaler())\n# ])\n\n# # Preprocessing for categorical data: impute missing values and one-hot encode\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# # Combine preprocessing steps\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_transformer, numerical_cols),\n#         ('cat', categorical_transformer, categorical_cols)\n#     ])\n\n# # Apply preprocessing to the features\n# X_preprocessed = preprocessor.fit_transform(X)\n\n# # Convert sparse matrix to dense\n# X_preprocessed = X_preprocessed.toarray()\n\n# # Split the dataset into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# # Build the Neural Network\n# model = Sequential()\n# model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n# model.add(Dense(64, activation='relu'))\n# model.add(Dense(32, activation='relu'))\n# model.add(Dense(1))\n\n# # Compile the model\n# model.compile(optimizer='adam', loss='mean_squared_error')\n\n# # Train the model\n# model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n\n# # Make predictions on the test set\n# y_pred = model.predict(X_test)\n\n# # Evaluate the model\n# mae = mean_absolute_error(y_test, y_pred)\n# mse = mean_squared_error(y_test, y_pred)\n# rmse = np.sqrt(mse)\n# r2 = r2_score(y_test, y_pred)\n\n# print(f'MAE: {mae}')\n# print(f'MSE: {mse}')\n# print(f'RMSE: {rmse}')\n# print(f'R2: {r2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:08.703854Z","iopub.execute_input":"2024-09-09T14:43:08.704209Z","iopub.status.idle":"2024-09-09T14:43:08.720070Z","shell.execute_reply.started":"2024-09-09T14:43:08.704155Z","shell.execute_reply":"2024-09-09T14:43:08.718941Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Support Vector Regression (SVR)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# Drop the 'Crop' column since it has only one type of crop\n# data = data.drop(columns=['Crop'])\n\n# Separate features and target variable\nX = data.drop(columns=['Yield'])\ny = data['Yield']\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year', 'Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the features\nX_preprocessed = preprocessor.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# Train the SVR model\nsvr = SVR(kernel='rbf')\nsvr.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = svr.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'MAE: {mae}')\nprint(f'MSE: {mse}')\nprint(f'RMSE: {rmse}')\nprint(f'R2: {r2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:20:08.970930Z","iopub.execute_input":"2024-10-08T06:20:08.971390Z","iopub.status.idle":"2024-10-08T06:20:09.068258Z","shell.execute_reply.started":"2024-10-08T06:20:08.971355Z","shell.execute_reply":"2024-10-08T06:20:09.066785Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gaussian Process Regression (GPR)","metadata":{}},{"cell_type":"code","source":"# # gpr on yield\n\n# import numpy as np\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# from sklearn.gaussian_process import GaussianProcessRegressor\n# from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\n# # Load the dataset\n# data = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# # Drop the 'Crop' column since it has only one type of crop\n# # data = data.drop(columns=['Crop'])\n\n# # Separate features and target variable\n# X = data.drop(columns=['Yield'])\n# y = data['Yield']\n\n# # Identify categorical and numerical columns\n# categorical_cols = ['Crop_Year', 'Season', 'State']\n# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# # Preprocessing for numerical data: impute missing values and scale features\n# numerical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean')),\n#     ('scaler', StandardScaler())\n# ])\n\n# # Preprocessing for categorical data: impute missing values and one-hot encode\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# # Combine preprocessing steps\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_transformer, numerical_cols),\n#         ('cat', categorical_transformer, categorical_cols)\n#     ])\n\n# # Apply preprocessing to the features\n# X_preprocessed = preprocessor.fit_transform(X)\n\n# # Convert the sparse matrix to a dense format\n# X_preprocessed = X_preprocessed.toarray()\n\n# # Split the dataset into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# # Define the kernel for GPR\n# kernel = C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))\n\n# # Create and fit the GPR model\n# gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n# gpr.fit(X_train, y_train)\n\n# # Make predictions on the test set\n# y_pred, y_std = gpr.predict(X_test, return_std=True)\n\n# # Evaluate the model\n# mae = mean_absolute_error(y_test, y_pred)\n# mse = mean_squared_error(y_test, y_pred)\n# rmse = np.sqrt(mse)\n# r2 = r2_score(y_test, y_pred)\n\n# print(f'MAE: {mae}')\n# print(f'MSE: {mse}')\n# print(f'RMSE: {rmse}')\n# print(f'R2: {r2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:20:10.677040Z","iopub.execute_input":"2024-10-08T06:20:10.677430Z","iopub.status.idle":"2024-10-08T06:20:10.685883Z","shell.execute_reply.started":"2024-10-08T06:20:10.677399Z","shell.execute_reply":"2024-10-08T06:20:10.684423Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # gpr on production\n\n# import numpy as np\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# from sklearn.gaussian_process import GaussianProcessRegressor\n# from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\n# # Load the dataset\n# data = pd.read_csv('/kaggle/working/merged_df.csv')\n\n# # Drop the 'Crop' column since it has only one type of crop\n# # data = data.drop(columns=['Crop'])\n\n# # Separate features and target variable\n# X = data.drop(columns=['Production'])\n# y = data['Production']\n\n# # Identify categorical and numerical columns\n# categorical_cols = ['Crop_Year', 'Season', 'State']\n# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# # Preprocessing for numerical data: impute missing values and scale features\n# numerical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean')),\n#     ('scaler', StandardScaler())\n# ])\n\n# # Preprocessing for categorical data: impute missing values and one-hot encode\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# # Combine preprocessing steps\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_transformer, numerical_cols),\n#         ('cat', categorical_transformer, categorical_cols)\n#     ])\n\n# # Apply preprocessing to the features\n# X_preprocessed = preprocessor.fit_transform(X)\n\n# # Convert the sparse matrix to a dense format\n# X_preprocessed = X_preprocessed.toarray()\n\n# # Split the dataset into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# # Define the kernel for GPR\n# kernel = C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))\n\n# # Create and fit the GPR model\n# gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n# gpr.fit(X_train, y_train)\n\n# # Make predictions on the test set\n# y_pred, y_std = gpr.predict(X_test, return_std=True)\n\n# # Evaluate the model\n# mae = mean_absolute_error(y_test, y_pred)\n# mse = mean_squared_error(y_test, y_pred)\n# rmse = np.sqrt(mse)\n# r2 = r2_score(y_test, y_pred)\n\n# print(f'MAE: {mae}')\n# print(f'MSE: {mse}')\n# print(f'RMSE: {rmse}')\n# print(f'R2: {r2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:08.827951Z","iopub.execute_input":"2024-09-09T14:43:08.828399Z","iopub.status.idle":"2024-09-09T14:43:08.844302Z","shell.execute_reply.started":"2024-09-09T14:43:08.828369Z","shell.execute_reply":"2024-09-09T14:43:08.843122Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"pip install xgboost","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:20:14.846463Z","iopub.execute_input":"2024-10-08T06:20:14.846901Z","iopub.status.idle":"2024-10-08T06:20:50.193620Z","shell.execute_reply.started":"2024-10-08T06:20:14.846866Z","shell.execute_reply":"2024-10-08T06:20:50.192183Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom xgboost import XGBRegressor\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# Separate features and target variable\nX = data.drop(columns=['Yield'])\ny = data['Yield']\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year', 'Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the features\nX_preprocessed = preprocessor.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# Initialize the XGBoost Regressor\nxgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n\n# Train the model\nxgb_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = xgb_model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:20:50.196323Z","iopub.execute_input":"2024-10-08T06:20:50.196751Z","iopub.status.idle":"2024-10-08T06:20:50.607605Z","shell.execute_reply.started":"2024-10-08T06:20:50.196711Z","shell.execute_reply":"2024-10-08T06:20:50.606564Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"# pip install lightgbm","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:41.300434Z","iopub.execute_input":"2024-09-09T14:43:41.301104Z","iopub.status.idle":"2024-09-09T14:43:41.305921Z","shell.execute_reply.started":"2024-09-09T14:43:41.301068Z","shell.execute_reply":"2024-09-09T14:43:41.304723Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# import lightgbm as lgb\n\n# # Load the dataset\n# data = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# # Separate features and target variable\n# X = data.drop(columns=['Yield'])\n# y = data['Yield']\n\n# # Identify categorical and numerical columns\n# categorical_cols = ['Crop_Year', 'Season', 'State']\n# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# # Preprocessing for numerical data: impute missing values and scale features\n# numerical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean')),\n#     ('scaler', StandardScaler())\n# ])\n\n# # Preprocessing for categorical data: impute missing values and one-hot encode\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# # Combine preprocessing steps\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_transformer, numerical_cols),\n#         ('cat', categorical_transformer, categorical_cols)\n#     ])\n\n# # Apply preprocessing to the features\n# X_preprocessed = preprocessor.fit_transform(X)\n\n# # Split the dataset into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# # Initialize the LightGBM Regressor\n# lgb_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n\n# # Train the model\n# lgb_model.fit(X_train, y_train)\n\n# # Make predictions on the test set\n# y_pred = lgb_model.predict(X_test)\n\n# # Evaluate the model\n# mae = mean_absolute_error(y_test, y_pred)\n# mse = mean_squared_error(y_test, y_pred)\n# rmse = np.sqrt(mse)\n# r2 = r2_score(y_test, y_pred)\n\n# print(f'Mean Absolute Error: {mae}')\n# print(f'Mean Squared Error: {mse}')\n# print(f'Root Mean Squared Error: {rmse}')\n# print(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:41.307457Z","iopub.execute_input":"2024-09-09T14:43:41.307786Z","iopub.status.idle":"2024-09-09T14:43:41.319618Z","shell.execute_reply.started":"2024-09-09T14:43:41.307731Z","shell.execute_reply":"2024-09-09T14:43:41.318455Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CATBoost","metadata":{}},{"cell_type":"code","source":"pip install catboost","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:20:59.735856Z","iopub.execute_input":"2024-10-08T06:20:59.736748Z","iopub.status.idle":"2024-10-08T06:21:35.113867Z","shell.execute_reply.started":"2024-10-08T06:20:59.736707Z","shell.execute_reply":"2024-10-08T06:21:35.112343Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom catboost import CatBoostRegressor\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# Separate features and target variable\nX = data.drop(columns=['Yield'])\ny = data['Yield']\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year', 'Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the features\nX_preprocessed = preprocessor.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# Initialize the CatBoost Regressor\ncatboost_model = CatBoostRegressor(iterations=100, learning_rate=0.1, random_state=42, verbose=0)\n\n# Train the model\ncatboost_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = catboost_model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:21:35.116524Z","iopub.execute_input":"2024-10-08T06:21:35.116936Z","iopub.status.idle":"2024-10-08T06:21:35.847511Z","shell.execute_reply.started":"2024-10-08T06:21:35.116899Z","shell.execute_reply":"2024-10-08T06:21:35.846206Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RNN","metadata":{}},{"cell_type":"code","source":"# pip install tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:44:14.140318Z","iopub.execute_input":"2024-09-09T14:44:14.140776Z","iopub.status.idle":"2024-09-09T14:44:14.145537Z","shell.execute_reply.started":"2024-09-09T14:44:14.140740Z","shell.execute_reply":"2024-09-09T14:44:14.144462Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, LSTM, Dropout\n\n# # Load the dataset\n# data = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# # Separate features and target variable\n# X = data.drop(columns=['Yield'])\n# y = data['Yield']\n\n# # Identify categorical and numerical columns\n# categorical_cols = ['Crop_Year', 'Season', 'State']\n# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# # Preprocessing for numerical data: impute missing values and scale features\n# numerical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean')),\n#     ('scaler', StandardScaler())\n# ])\n\n# # Preprocessing for categorical data: impute missing values and one-hot encode\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# # Combine preprocessing steps\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_transformer, numerical_cols),\n#         ('cat', categorical_transformer, categorical_cols)\n#     ])\n\n# # Apply preprocessing to the features\n# X_preprocessed = preprocessor.fit_transform(X)\n\n# # Convert the sparse matrix to a dense one\n# X_preprocessed = X_preprocessed.toarray()\n\n# # Reshape the data to fit the LSTM input format (samples, time steps, features)\n# # Here we assume each sample is one time step\n# X_preprocessed = X_preprocessed.reshape((X_preprocessed.shape[0], 1, X_preprocessed.shape[1]))\n\n# # Split the dataset into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# # Define the RNN model\n# model = Sequential()\n# model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n# model.add(Dropout(0.2))\n# model.add(Dense(1))\n# model.compile(optimizer='adam', loss='mean_squared_error')\n\n# # Train the model\n# model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n\n# # Make predictions on the test set\n# y_pred = model.predict(X_test)\n\n# # Evaluate the model\n# mae = mean_absolute_error(y_test, y_pred)\n# mse = mean_squared_error(y_test, y_pred)\n# rmse = np.sqrt(mse)\n# r2 = r2_score(y_test, y_pred)\n\n# print(f'Mean Absolute Error: {mae}')\n# print(f'Mean Squared Error: {mse}')\n# print(f'Root Mean Squared Error: {rmse}')\n# print(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:44:14.147113Z","iopub.execute_input":"2024-09-09T14:44:14.147637Z","iopub.status.idle":"2024-09-09T14:44:14.166916Z","shell.execute_reply.started":"2024-09-09T14:44:14.147600Z","shell.execute_reply":"2024-09-09T14:44:14.165866Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next things to try:\n1. drop yield and get results with production as output (not needed)\n2. drop production and get results with yield as output (done)\n3. apply random forest on a set of features to analyse most imp combo of features","metadata":{}},{"cell_type":"code","source":"# cross validation on random forest \n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# Drop the 'Crop' column since it has only one type of crop\n# data = data.drop(columns=['Crop'])\n\n# Separate features and target variable\nX = data.drop(columns=['Yield'])\ny = data['Yield']\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year', 'Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the features\nX_preprocessed = preprocessor.fit_transform(X)\n\n# Define the cross-validation strategy\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Initialize the Random Forest Regressor\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Define scoring metrics\nscoring = {\n    'MAE': make_scorer(mean_absolute_error),\n    'MSE': make_scorer(mean_squared_error),\n    'R2': make_scorer(r2_score)\n}\n\n# Perform cross-validation and get the scores\ncv_results = cross_validate(rf_model, X_preprocessed, y, cv=kf, scoring=scoring, return_train_score=True)\n\n# Display cross-validation results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean Absolute Error: {np.mean(cv_results['test_MAE'])}\")\nprint(f\"Mean Squared Error: {np.mean(cv_results['test_MSE'])}\")\nprint(f\"R2 Score: {np.mean(cv_results['test_R2'])}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:21:57.696231Z","iopub.execute_input":"2024-10-08T06:21:57.697670Z","iopub.status.idle":"2024-10-08T06:22:05.593105Z","shell.execute_reply.started":"2024-10-08T06:21:57.697625Z","shell.execute_reply":"2024-10-08T06:22:05.591962Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df_1.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:05.594802Z","iopub.execute_input":"2024-10-08T06:22:05.595132Z","iopub.status.idle":"2024-10-08T06:22:05.619501Z","shell.execute_reply.started":"2024-10-08T06:22:05.595104Z","shell.execute_reply":"2024-10-08T06:22:05.618241Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df_1.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:08.958656Z","iopub.execute_input":"2024-10-08T06:22:08.959254Z","iopub.status.idle":"2024-10-08T06:22:08.973237Z","shell.execute_reply.started":"2024-10-08T06:22:08.959216Z","shell.execute_reply":"2024-10-08T06:22:08.972078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comparing different attributes with RF","metadata":{}},{"cell_type":"code","source":"# total 14 input attributes\n# ignoring the categorical features Crop_Year, Season, State\n# now have 11 options\n# making random attribute sets from these 11 to find most imp attributes\n\nset_output=merged_df_1['Yield']","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:12.070923Z","iopub.execute_input":"2024-10-08T06:22:12.071374Z","iopub.status.idle":"2024-10-08T06:22:12.077321Z","shell.execute_reply.started":"2024-10-08T06:22:12.071337Z","shell.execute_reply":"2024-10-08T06:22:12.075918Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attr_set_1=['Crop_Year','Season', 'State','temperature','ph','rainfall','Fertilizer','Pesticide']\nset_1=merged_df_1[attr_set_1].copy()\n\n# Separate features and target variable\nX = set_1\ny = set_output\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year','Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the training and testing data\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\n\n# Initialize the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:12.833061Z","iopub.execute_input":"2024-10-08T06:22:12.833524Z","iopub.status.idle":"2024-10-08T06:22:13.957799Z","shell.execute_reply.started":"2024-10-08T06:22:12.833484Z","shell.execute_reply":"2024-10-08T06:22:13.956300Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attr_set_2=['Crop_Year','Season', 'State','N','P','K','humidity']\nset_2=merged_df_1[attr_set_2].copy()\n# Separate features and target variable\nX = set_2\ny = set_output\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year','Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the training and testing data\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\n\n# Initialize the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:13.960842Z","iopub.execute_input":"2024-10-08T06:22:13.961336Z","iopub.status.idle":"2024-10-08T06:22:14.957695Z","shell.execute_reply.started":"2024-10-08T06:22:13.961294Z","shell.execute_reply":"2024-10-08T06:22:14.956381Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attr_set_3=['Crop_Year','Season', 'State', 'temperature', 'ph', 'Fertilizer', 'N','P','K']\nset_3=merged_df_1[attr_set_3].copy()\n\n# Separate features and target variable\nX = set_3\ny = set_output\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year','Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the training and testing data\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\n\n# Initialize the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:14.959531Z","iopub.execute_input":"2024-10-08T06:22:14.960028Z","iopub.status.idle":"2024-10-08T06:22:16.156957Z","shell.execute_reply.started":"2024-10-08T06:22:14.959969Z","shell.execute_reply":"2024-10-08T06:22:16.155566Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attr_set_4=['Crop_Year','Season', 'State', 'temperature', 'ph', 'N','P','K']\nset_4=merged_df_1[attr_set_4].copy()\n\n# Separate features and target variable\nX = set_4\ny = set_output\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year','Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the training and testing data\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\n\n# Initialize the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:16.159242Z","iopub.execute_input":"2024-10-08T06:22:16.159665Z","iopub.status.idle":"2024-10-08T06:22:17.229349Z","shell.execute_reply.started":"2024-10-08T06:22:16.159630Z","shell.execute_reply":"2024-10-08T06:22:17.227701Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PLOTTING MODEL COMPARISONS","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Define the attribute sets and their corresponding R2 scores\nattr_sets = ['Attr Set 1', 'Attr Set 2', 'Attr Set 3', 'Attr Set 4']\nr2_scores = [0.9534964916495487, 0.9011542128032501, 0.9359800615312223, 0.8927537138619698]\n\n# Create a figure and axis\nplt.figure(figsize=(10, 6))\n\n# Plot the R2 scores for different attribute sets\nplt.plot(attr_sets, r2_scores, marker='o', linestyle='-', color='blue', label='R2 Score')\n\n# Annotate points with their R2 score values\nfor i, score in enumerate(r2_scores):\n    plt.text(attr_sets[i], score, f'{score:.4f}', ha='center', va='bottom')\n\n# Title and labels\nplt.title('R2 Scores for Different Attribute Sets Using Random Forest')\nplt.ylabel('R2 Score')\nplt.xlabel('Attribute Sets')\nplt.ylim(0, 1)  # Set y-axis limit for better visualization\nplt.legend()\nplt.grid(True)\n\n# Save the figure\nplt.savefig('/kaggle/working/RF_comparisons.png')\n\n# Display the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:18.023512Z","iopub.execute_input":"2024-10-08T06:22:18.024539Z","iopub.status.idle":"2024-10-08T06:22:18.489754Z","shell.execute_reply.started":"2024-10-08T06:22:18.024489Z","shell.execute_reply":"2024-10-08T06:22:18.488527Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Define the attribute sets and their corresponding R2 scores\nattr_sets = ['Attr Set 1', 'Attr Set 2', 'Attr Set 3', 'Attr Set 4']\nr2_scores = [0.9534964916495487, 0.9011542128032501, 0.9359800615312223, 0.8927537138619698]\n\n# Define hatch patterns for each bar\npatterns = ['/', '\\\\', '|', '-']\n\n# Create a smaller figure and axis\nplt.figure(figsize=(10, 6))\n\n# Plot the R2 scores as a bar chart with different patterns\nbars = plt.bar(attr_sets, r2_scores, hatch=patterns[0], color='white', edgecolor='black')\n\n# Apply different hatch patterns to each bar\nfor bar, pattern in zip(bars, patterns):\n    bar.set_hatch(pattern)\n\n# Annotate bars with their R2 score values (smaller font and close to bars)\nfor i, score in enumerate(r2_scores):\n    plt.text(i, score + 0.005, f'{score:.4f}', ha='center', va='bottom', fontsize=8)\n\n# Title and labels (smaller font size)\n\nplt.ylabel('R2 Score', fontsize=9)\nplt.xlabel('Attribute Sets', fontsize=9)\nplt.ylim(0.8, 1)  # Set y-axis limit to focus on the range 0.8 to 1\nplt.grid(False)\n\n# Save the figure\nplt.savefig('/kaggle/working/RF_comparisons_bar.png')\n\n# Display the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:57:11.851890Z","iopub.execute_input":"2024-10-22T17:57:11.852493Z","iopub.status.idle":"2024-10-22T17:57:12.265517Z","shell.execute_reply.started":"2024-10-22T17:57:11.852446Z","shell.execute_reply":"2024-10-22T17:57:12.264098Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Results from different models\nmodels = ['Random Forest', 'XGBoost', 'SVR', 'CatBoost']\nr2_scores = [0.9550701929644764, 0.9436145383374764, 0.6003614289271699, 0.1771460132644208]\n\n# Create the line plot for R2 scores\nplt.figure(figsize=(10, 6))\n\n# Plot each model's R2 score with individual lines\nplt.plot(models, r2_scores, marker='o', linestyle='-', color='b', label='R2 Score')\n\n# Highlight individual scores with markers\nfor i, score in enumerate(r2_scores):\n    plt.text(models[i], score, f'{score:.2f}', ha='center', va='bottom')\n\nplt.title('R2 Scores of Different Models')\nplt.ylabel('R2 Score')\nplt.ylim(0, 1)\nplt.xlabel('Models')\nplt.grid(axis='y')\nplt.legend()\n\n# Display the plot\nplt.savefig('/kaggle/working/r2_scores.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:29.120827Z","iopub.execute_input":"2024-10-08T06:22:29.121263Z","iopub.status.idle":"2024-10-08T06:22:29.578334Z","shell.execute_reply.started":"2024-10-08T06:22:29.121229Z","shell.execute_reply":"2024-10-08T06:22:29.576860Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Metrics\nmetrics = ['MAE', 'MSE', 'RMSE', 'R2 Score']\n\n# Results for each model\nresults = {\n    'Random Forest': [0.203, 0.107, 0.327, 0.955],\n    'XGBoost': [0.225, 0.134, 0.366, 0.944],\n    'SVR': [0.531, 0.952, 0.976, 0.600],\n    'CatBoost': [0.932, 1.960, 1.400, 0.177]\n}\n\n# Create a figure and axis\nplt.figure(figsize=(12, 8))\n\n# Plot each model's metrics\nfor model, values in results.items():\n    plt.plot(metrics, values, marker='o', linestyle='-', label=model)\n\n# Annotate points with their values\nfor model, values in results.items():\n    for i, value in enumerate(values):\n        plt.text(metrics[i], value, f'{value:.3f}', ha='center', va='bottom')\n\n# Title and labels\nplt.title('Model Performance Across Different Metrics')\nplt.ylabel('Metric Value')\nplt.xlabel('Metrics')\nplt.legend()\nplt.grid(True)\n\n# Display the plot\nplt.savefig('/kaggle/working/model_performances.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:26:06.531634Z","iopub.execute_input":"2024-10-08T06:26:06.532142Z","iopub.status.idle":"2024-10-08T06:26:07.174869Z","shell.execute_reply.started":"2024-10-08T06:26:06.532104Z","shell.execute_reply":"2024-10-08T06:26:07.173540Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Metrics\nmetrics = ['MAE', 'MSE', 'RMSE', 'R2 Score']\n\n# Results for each model\nresults = {\n    'Random Forest': [0.203, 0.107, 0.327, 0.955],\n    'XGBoost': [0.225, 0.134, 0.366, 0.944],\n    'SVR': [0.531, 0.952, 0.976, 0.600],\n    'CatBoost': [0.932, 1.960, 1.400, 0.177]\n}\n\n# Define the number of models and metrics\nn_models = len(results)\nn_metrics = len(metrics)\n\n# Set up bar width and index positions for each group of bars\nbar_width = 0.2\nindex = np.arange(n_metrics)\n\n# Create a figure and axis\nplt.figure(figsize=(12, 8))\n\n# Plot each model's metrics as bars\nfor i, (model, values) in enumerate(results.items()):\n    plt.bar(index + i * bar_width, values, bar_width, label=model)\n\n# Annotate bars with their values\nfor i, (model, values) in enumerate(results.items()):\n    for j, value in enumerate(values):\n        plt.text(index[j] + i * bar_width, value + 0.02, f'{value:.3f}', ha='center', va='bottom')\n\n# Title and labels\nplt.title('Model Performance Across Different Metrics')\nplt.ylabel('Metric Value')\nplt.xlabel('Metrics')\nplt.xticks(index + bar_width * (n_models - 1) / 2, metrics)\nplt.legend()\nplt.grid(True, axis='y')\n\n# Display the plot\nplt.savefig('/kaggle/working/model_performances_bar.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:30:06.376368Z","iopub.execute_input":"2024-10-08T06:30:06.377400Z","iopub.status.idle":"2024-10-08T06:30:06.999178Z","shell.execute_reply.started":"2024-10-08T06:30:06.377353Z","shell.execute_reply":"2024-10-08T06:30:06.997892Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Results for each model\nresults = {\n    'Random Forest': 0.203,\n    'XGBoost': 0.225,\n    'SVR': 0.531,\n    'CatBoost': 0.932\n}\n\n# Define consistent hatch patterns for each model\npatterns = ['/', '..', 'x', '--']\n\n# Create a bar plot for MAE\nplt.figure(figsize=(8, 5), facecolor='white')  # Set figure background to white\nbars = plt.bar(results.keys(), results.values(), edgecolor='black', hatch=patterns, color='none')\n\n# Annotate bars with their values\nfor bar, value in zip(bars, results.values()):\n    plt.text(bar.get_x() + bar.get_width() / 2, value + 0.02, f'{value:.3f}', ha='center', va='bottom')\n\n# Labels\nplt.ylabel('MAE Value')\nplt.xlabel('Models')\nplt.ylim(0, 1)  # Set y-axis limit for better visualization\n\n# Remove grid lines\nplt.grid(False)\n\n# Save and display the plot\nplt.savefig('/kaggle/working/MAE_models_bar.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:50:34.929642Z","iopub.execute_input":"2024-10-22T17:50:34.930090Z","iopub.status.idle":"2024-10-22T17:50:35.352930Z","shell.execute_reply.started":"2024-10-22T17:50:34.930056Z","shell.execute_reply":"2024-10-22T17:50:35.351431Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Results for each model\nresults = {\n    'Random Forest': 0.107,\n    'XGBoost': 0.134,\n    'SVR': 0.952,\n    'CatBoost': 1.960\n}\n\n# Define consistent hatch patterns for each model\npatterns = ['/', '..', 'x', '--']\n\n# Create a bar plot for MSE\nplt.figure(figsize=(8, 5), facecolor='white')  # Set figure background to white\nbars = plt.bar(results.keys(), results.values(), edgecolor='black', hatch=patterns, color='none')\n\n# Annotate bars with their values\nfor bar, value in zip(bars, results.values()):\n    plt.text(bar.get_x() + bar.get_width() / 2, value + 0.05, f'{value:.3f}', ha='center', va='bottom')\n\n# Title and labels\n\nplt.ylabel('MSE Value')\nplt.xlabel('Models')\nplt.ylim(0, 2.5)  # Set y-axis limit for better visualization\nplt.grid(False)\n\n# Save and display the plot\nplt.savefig('/kaggle/working/MSE_models_bar.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:51:19.109875Z","iopub.execute_input":"2024-10-22T17:51:19.110888Z","iopub.status.idle":"2024-10-22T17:51:19.464274Z","shell.execute_reply.started":"2024-10-22T17:51:19.110832Z","shell.execute_reply":"2024-10-22T17:51:19.462886Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Results for each model\nresults = {\n    'Random Forest': 0.327,\n    'XGBoost': 0.366,\n    'SVR': 0.976,\n    'CatBoost': 1.400\n}\n\n# Define consistent hatch patterns for each model\npatterns = ['/', '..', 'x', '--']\n\n# Create a bar plot for RMSE\nplt.figure(figsize=(8, 5), facecolor='white')  # Set figure background to white\nbars = plt.bar(results.keys(), results.values(), edgecolor='black', hatch=patterns, color='none')\n\n# Annotate bars with their values\nfor bar, value in zip(bars, results.values()):\n    plt.text(bar.get_x() + bar.get_width() / 2, value + 0.05, f'{value:.3f}', ha='center', va='bottom')\n\n# Title and labels\n\nplt.ylabel('RMSE Value')\nplt.xlabel('Models')\nplt.ylim(0, 1.6)  # Set y-axis limit for better visualization\nplt.grid(False)\n\n# Save and display the plot\nplt.savefig('/kaggle/working/RMSE_models_bar.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:51:27.719358Z","iopub.execute_input":"2024-10-22T17:51:27.719823Z","iopub.status.idle":"2024-10-22T17:51:28.088412Z","shell.execute_reply.started":"2024-10-22T17:51:27.719789Z","shell.execute_reply":"2024-10-22T17:51:28.087164Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Results for each model\nresults = {\n    'Random Forest': 0.955,\n    'XGBoost': 0.944,\n    'SVR': 0.600,\n    'CatBoost': 0.177\n}\n\n# Define consistent hatch patterns for each model\npatterns = ['/', '..', 'x', '--']\n\n# Create a bar plot for R Score\nplt.figure(figsize=(8, 5), facecolor='white')  # Set figure background to white\nbars = plt.bar(results.keys(), results.values(), edgecolor='black', hatch=patterns, color='none')\n\n# Annotate bars with their values\nfor bar, value in zip(bars, results.values()):\n    plt.text(bar.get_x() + bar.get_width() / 2, value + 0.02, f'{value:.3f}', ha='center', va='bottom')\n\n# Title and labels\n\nplt.ylabel('R Score Value')\nplt.xlabel('Models')\nplt.ylim(0, 1.1)  # Set y-axis limit for better visualization\nplt.grid(False)\n\n# Save and display the plot\nplt.savefig('/kaggle/working/R2_models_bar.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:52:41.672758Z","iopub.execute_input":"2024-10-22T17:52:41.673749Z","iopub.status.idle":"2024-10-22T17:52:42.040323Z","shell.execute_reply.started":"2024-10-22T17:52:41.673708Z","shell.execute_reply":"2024-10-22T17:52:42.039036Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom IPython.display import display, Markdown\n\n# Define the data\ndata = {\n    'Model': ['Random Forest', 'XGBoost', 'SVR', 'CatBoost'],\n    'Mean Absolute Error': [0.2031, 0.2254, 0.5311, 0.9317],\n    'Mean Squared Error': [0.1070, 0.1343, 0.9520, 1.9602],\n    'Root Mean Squared Error': [0.3272, 0.3665, 0.9757, 1.4001],\n    'R2 Score': [0.9551, 0.9436, 0.6004, 0.1771]\n}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Display the table in a readable format using markdown\ntable_md = df.to_markdown(index=False)\ndisplay(Markdown(table_md))\n\n# Save the DataFrame to a CSV file\n# df.to_csv('/kaggle/working/model_performance_table.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:44:27.796547Z","iopub.execute_input":"2024-09-09T14:44:27.799568Z","iopub.status.idle":"2024-09-09T14:44:27.865503Z","shell.execute_reply.started":"2024-09-09T14:44:27.799524Z","shell.execute_reply":"2024-09-09T14:44:27.864331Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}