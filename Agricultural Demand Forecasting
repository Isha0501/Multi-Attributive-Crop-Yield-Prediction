{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7247975,"sourceType":"datasetVersion","datasetId":4199008},{"sourceId":7815897,"sourceType":"datasetVersion","datasetId":4578757},{"sourceId":8666227,"sourceType":"datasetVersion","datasetId":5193159}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ishanevatia/agro-demand-forecasting?scriptVersionId=237053205\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-04-30T14:26:33.099822Z","iopub.execute_input":"2025-04-30T14:26:33.100245Z","iopub.status.idle":"2025-04-30T14:26:33.649853Z","shell.execute_reply.started":"2025-04-30T14:26:33.100195Z","shell.execute_reply":"2025-04-30T14:26:33.648485Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/crop-yield/crop_yield.csv\n/kaggle/input/crop-recommendation/Crop_recommendation.csv\n/kaggle/input/india-crop-yield/Crop_Production_Statistics.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#this dataset is giving us the conditions required to grow a particular crop\n#no info on where these conditions are found\n# Crop_Production_Statistics does not have rainfall, thus using crop_yield\n\nsoil=pd.read_csv('/kaggle/input/crop-recommendation/Crop_recommendation.csv')\ncrops=pd.read_csv('/kaggle/input/crop-yield/crop_yield.csv')\n# crops=pd.read_csv('/kaggle/input/india-crop-yield/Crop_Production_Statistics.csv')","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:26:33.651431Z","iopub.execute_input":"2025-04-30T14:26:33.651896Z","iopub.status.idle":"2025-04-30T14:26:33.75135Z","shell.execute_reply.started":"2025-04-30T14:26:33.65185Z","shell.execute_reply":"2025-04-30T14:26:33.749914Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"soil.head()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2025-04-30T14:26:47.519301Z","iopub.execute_input":"2025-04-30T14:26:47.520518Z","iopub.status.idle":"2025-04-30T14:26:47.541991Z","shell.execute_reply.started":"2025-04-30T14:26:47.520477Z","shell.execute_reply":"2025-04-30T14:26:47.54051Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"    N   P   K  temperature   humidity        ph    rainfall label\n0  90  42  43    20.879744  82.002744  6.502985  202.935536  rice\n1  85  58  41    21.770462  80.319644  7.038096  226.655537  rice\n2  60  55  44    23.004459  82.320763  7.840207  263.964248  rice\n3  74  35  40    26.491096  80.158363  6.980401  242.864034  rice\n4  78  42  42    20.130175  81.604873  7.628473  262.717340  rice","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N</th>\n      <th>P</th>\n      <th>K</th>\n      <th>temperature</th>\n      <th>humidity</th>\n      <th>ph</th>\n      <th>rainfall</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>90</td>\n      <td>42</td>\n      <td>43</td>\n      <td>20.879744</td>\n      <td>82.002744</td>\n      <td>6.502985</td>\n      <td>202.935536</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>85</td>\n      <td>58</td>\n      <td>41</td>\n      <td>21.770462</td>\n      <td>80.319644</td>\n      <td>7.038096</td>\n      <td>226.655537</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60</td>\n      <td>55</td>\n      <td>44</td>\n      <td>23.004459</td>\n      <td>82.320763</td>\n      <td>7.840207</td>\n      <td>263.964248</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>74</td>\n      <td>35</td>\n      <td>40</td>\n      <td>26.491096</td>\n      <td>80.158363</td>\n      <td>6.980401</td>\n      <td>242.864034</td>\n      <td>rice</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>78</td>\n      <td>42</td>\n      <td>42</td>\n      <td>20.130175</td>\n      <td>81.604873</td>\n      <td>7.628473</td>\n      <td>262.717340</td>\n      <td>rice</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"crops.head()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:27:54.906842Z","iopub.execute_input":"2025-04-30T14:27:54.907442Z","iopub.status.idle":"2025-04-30T14:27:54.931059Z","shell.execute_reply.started":"2025-04-30T14:27:54.907398Z","shell.execute_reply":"2025-04-30T14:27:54.929703Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           Crop  Crop_Year       Season  State     Area  Production  \\\n0      Arecanut       1997  Whole Year   Assam  73814.0       56708   \n1     Arhar/Tur       1997  Kharif       Assam   6637.0        4685   \n2   Castor seed       1997  Kharif       Assam    796.0          22   \n3      Coconut        1997  Whole Year   Assam  19656.0   126905000   \n4  Cotton(lint)       1997  Kharif       Assam   1739.0         794   \n\n   Annual_Rainfall  Fertilizer  Pesticide        Yield  \n0           2051.4  7024878.38   22882.34     0.796087  \n1           2051.4   631643.29    2057.47     0.710435  \n2           2051.4    75755.32     246.76     0.238333  \n3           2051.4  1870661.52    6093.36  5238.051739  \n4           2051.4   165500.63     539.09     0.420909  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Crop</th>\n      <th>Crop_Year</th>\n      <th>Season</th>\n      <th>State</th>\n      <th>Area</th>\n      <th>Production</th>\n      <th>Annual_Rainfall</th>\n      <th>Fertilizer</th>\n      <th>Pesticide</th>\n      <th>Yield</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Arecanut</td>\n      <td>1997</td>\n      <td>Whole Year</td>\n      <td>Assam</td>\n      <td>73814.0</td>\n      <td>56708</td>\n      <td>2051.4</td>\n      <td>7024878.38</td>\n      <td>22882.34</td>\n      <td>0.796087</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Arhar/Tur</td>\n      <td>1997</td>\n      <td>Kharif</td>\n      <td>Assam</td>\n      <td>6637.0</td>\n      <td>4685</td>\n      <td>2051.4</td>\n      <td>631643.29</td>\n      <td>2057.47</td>\n      <td>0.710435</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Castor seed</td>\n      <td>1997</td>\n      <td>Kharif</td>\n      <td>Assam</td>\n      <td>796.0</td>\n      <td>22</td>\n      <td>2051.4</td>\n      <td>75755.32</td>\n      <td>246.76</td>\n      <td>0.238333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Coconut</td>\n      <td>1997</td>\n      <td>Whole Year</td>\n      <td>Assam</td>\n      <td>19656.0</td>\n      <td>126905000</td>\n      <td>2051.4</td>\n      <td>1870661.52</td>\n      <td>6093.36</td>\n      <td>5238.051739</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cotton(lint)</td>\n      <td>1997</td>\n      <td>Kharif</td>\n      <td>Assam</td>\n      <td>1739.0</td>\n      <td>794</td>\n      <td>2051.4</td>\n      <td>165500.63</td>\n      <td>539.09</td>\n      <td>0.420909</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"soil['rainfall']=np.ceil(soil['rainfall'])","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:28:40.104942Z","iopub.execute_input":"2025-04-30T14:28:40.105349Z","iopub.status.idle":"2025-04-30T14:28:40.113071Z","shell.execute_reply.started":"2025-04-30T14:28:40.105318Z","shell.execute_reply":"2025-04-30T14:28:40.111864Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"crops['State'].nunique()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:28:46.051913Z","iopub.execute_input":"2025-04-30T14:28:46.052275Z","iopub.status.idle":"2025-04-30T14:28:46.065012Z","shell.execute_reply.started":"2025-04-30T14:28:46.052242Z","shell.execute_reply":"2025-04-30T14:28:46.063599Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"crops.State.unique()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:28:49.135206Z","iopub.execute_input":"2025-04-30T14:28:49.136403Z","iopub.status.idle":"2025-04-30T14:28:49.144198Z","shell.execute_reply.started":"2025-04-30T14:28:49.136331Z","shell.execute_reply":"2025-04-30T14:28:49.142959Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array(['Assam', 'Karnataka', 'Kerala', 'Meghalaya', 'West Bengal',\n       'Puducherry', 'Goa', 'Andhra Pradesh', 'Tamil Nadu', 'Odisha',\n       'Bihar', 'Gujarat', 'Madhya Pradesh', 'Maharashtra', 'Mizoram',\n       'Punjab', 'Uttar Pradesh', 'Haryana', 'Himachal Pradesh',\n       'Tripura', 'Nagaland', 'Chhattisgarh', 'Uttarakhand', 'Jharkhand',\n       'Delhi', 'Manipur', 'Jammu and Kashmir', 'Telangana',\n       'Arunachal Pradesh', 'Sikkim'], dtype=object)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"crops.Crop.unique()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:28:52.949736Z","iopub.execute_input":"2025-04-30T14:28:52.950117Z","iopub.status.idle":"2025-04-30T14:28:52.959019Z","shell.execute_reply.started":"2025-04-30T14:28:52.950088Z","shell.execute_reply":"2025-04-30T14:28:52.957867Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array(['Arecanut', 'Arhar/Tur', 'Castor seed', 'Coconut ', 'Cotton(lint)',\n       'Dry chillies', 'Gram', 'Jute', 'Linseed', 'Maize', 'Mesta',\n       'Niger seed', 'Onion', 'Other  Rabi pulses', 'Potato',\n       'Rapeseed &Mustard', 'Rice', 'Sesamum', 'Small millets',\n       'Sugarcane', 'Sweet potato', 'Tapioca', 'Tobacco', 'Turmeric',\n       'Wheat', 'Bajra', 'Black pepper', 'Cardamom', 'Coriander',\n       'Garlic', 'Ginger', 'Groundnut', 'Horse-gram', 'Jowar', 'Ragi',\n       'Cashewnut', 'Banana', 'Soyabean', 'Barley', 'Khesari', 'Masoor',\n       'Moong(Green Gram)', 'Other Kharif pulses', 'Safflower',\n       'Sannhamp', 'Sunflower', 'Urad', 'Peas & beans (Pulses)',\n       'other oilseeds', 'Other Cereals', 'Cowpea(Lobia)',\n       'Oilseeds total', 'Guar seed', 'Other Summer Pulses', 'Moth'],\n      dtype=object)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"crops['mean_rainfall']=np.ceil(crops['Annual_Rainfall']/12)\ncrops.head()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:28:56.683636Z","iopub.execute_input":"2025-04-30T14:28:56.684004Z","iopub.status.idle":"2025-04-30T14:28:56.703008Z","shell.execute_reply.started":"2025-04-30T14:28:56.683979Z","shell.execute_reply":"2025-04-30T14:28:56.701859Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"           Crop  Crop_Year       Season  State     Area  Production  \\\n0      Arecanut       1997  Whole Year   Assam  73814.0       56708   \n1     Arhar/Tur       1997  Kharif       Assam   6637.0        4685   \n2   Castor seed       1997  Kharif       Assam    796.0          22   \n3      Coconut        1997  Whole Year   Assam  19656.0   126905000   \n4  Cotton(lint)       1997  Kharif       Assam   1739.0         794   \n\n   Annual_Rainfall  Fertilizer  Pesticide        Yield  mean_rainfall  \n0           2051.4  7024878.38   22882.34     0.796087          171.0  \n1           2051.4   631643.29    2057.47     0.710435          171.0  \n2           2051.4    75755.32     246.76     0.238333          171.0  \n3           2051.4  1870661.52    6093.36  5238.051739          171.0  \n4           2051.4   165500.63     539.09     0.420909          171.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Crop</th>\n      <th>Crop_Year</th>\n      <th>Season</th>\n      <th>State</th>\n      <th>Area</th>\n      <th>Production</th>\n      <th>Annual_Rainfall</th>\n      <th>Fertilizer</th>\n      <th>Pesticide</th>\n      <th>Yield</th>\n      <th>mean_rainfall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Arecanut</td>\n      <td>1997</td>\n      <td>Whole Year</td>\n      <td>Assam</td>\n      <td>73814.0</td>\n      <td>56708</td>\n      <td>2051.4</td>\n      <td>7024878.38</td>\n      <td>22882.34</td>\n      <td>0.796087</td>\n      <td>171.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Arhar/Tur</td>\n      <td>1997</td>\n      <td>Kharif</td>\n      <td>Assam</td>\n      <td>6637.0</td>\n      <td>4685</td>\n      <td>2051.4</td>\n      <td>631643.29</td>\n      <td>2057.47</td>\n      <td>0.710435</td>\n      <td>171.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Castor seed</td>\n      <td>1997</td>\n      <td>Kharif</td>\n      <td>Assam</td>\n      <td>796.0</td>\n      <td>22</td>\n      <td>2051.4</td>\n      <td>75755.32</td>\n      <td>246.76</td>\n      <td>0.238333</td>\n      <td>171.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Coconut</td>\n      <td>1997</td>\n      <td>Whole Year</td>\n      <td>Assam</td>\n      <td>19656.0</td>\n      <td>126905000</td>\n      <td>2051.4</td>\n      <td>1870661.52</td>\n      <td>6093.36</td>\n      <td>5238.051739</td>\n      <td>171.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cotton(lint)</td>\n      <td>1997</td>\n      <td>Kharif</td>\n      <td>Assam</td>\n      <td>1739.0</td>\n      <td>794</td>\n      <td>2051.4</td>\n      <td>165500.63</td>\n      <td>539.09</td>\n      <td>0.420909</td>\n      <td>171.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"df = pd.DataFrame(soil)\n\n# Filter the dataframe\nsoil_filtered_df = df[(df['label']=='maize')]\n\n# Print the filtered dataframe\nprint(soil_filtered_df)","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:29:22.69762Z","iopub.execute_input":"2025-04-30T14:29:22.697987Z","iopub.status.idle":"2025-04-30T14:29:22.75263Z","shell.execute_reply.started":"2025-04-30T14:29:22.697961Z","shell.execute_reply":"2025-04-30T14:29:22.751243Z"},"trusted":true},"outputs":[{"name":"stdout","text":"      N   P   K  temperature   humidity        ph  rainfall  label\n100  71  54  16    22.613600  63.690706  5.749914      88.0  maize\n101  61  44  17    26.100184  71.574769  6.931757     103.0  maize\n102  80  43  16    23.558821  71.593514  6.657965      67.0  maize\n103  73  58  21    19.972160  57.682729  6.596061      61.0  maize\n104  61  38  20    18.478913  62.695039  5.970458      66.0  maize\n..   ..  ..  ..          ...        ...       ...       ...    ...\n195  90  57  24    18.928519  72.800861  6.158860      83.0  maize\n196  67  35  22    23.305468  63.246480  6.385684     109.0  maize\n197  60  54  19    18.748267  62.498785  6.417820      71.0  maize\n198  83  58  23    19.742133  59.662631  6.381202      66.0  maize\n199  83  57  19    25.730444  70.747393  6.877869      99.0  maize\n\n[100 rows x 8 columns]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"soil.label.unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T14:30:00.044801Z","iopub.execute_input":"2025-04-30T14:30:00.045189Z","iopub.status.idle":"2025-04-30T14:30:00.05529Z","shell.execute_reply.started":"2025-04-30T14:30:00.04516Z","shell.execute_reply":"2025-04-30T14:30:00.053312Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array(['rice', 'maize', 'chickpea', 'kidneybeans', 'pigeonpeas',\n       'mothbeans', 'mungbean', 'blackgram', 'lentil', 'pomegranate',\n       'banana', 'mango', 'grapes', 'watermelon', 'muskmelon', 'apple',\n       'orange', 'papaya', 'coconut', 'cotton', 'jute', 'coffee'],\n      dtype=object)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"df = pd.DataFrame(crops)\n\n# Filter the dataframe\ncrop_filtered_df = df[(df['Crop']=='Maize')]\n\n# Print the filtered dataframe\nprint(crop_filtered_df)","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:31:11.708508Z","iopub.execute_input":"2025-04-30T14:31:11.708881Z","iopub.status.idle":"2025-04-30T14:31:11.725405Z","shell.execute_reply.started":"2025-04-30T14:31:11.708855Z","shell.execute_reply":"2025-04-30T14:31:11.724122Z"},"trusted":true},"outputs":[{"name":"stdout","text":"        Crop  Crop_Year       Season              State      Area  Production  \\\n9      Maize       1997  Kharif                   Assam   19216.0       14721   \n45     Maize       1997  Kharif               Karnataka  502797.0     1391132   \n46     Maize       1997  Rabi                 Karnataka   48844.0       98932   \n47     Maize       1997  Summer               Karnataka    9730.0       20893   \n72     Maize       1997  Kharif               Meghalaya   17175.0       24878   \n...      ...        ...          ...                ...       ...         ...   \n19658  Maize       2018  Autumn                  Odisha   49126.0      113342   \n19659  Maize       2018  Summer                  Odisha    2855.0        7452   \n19660  Maize       2018  Winter                  Odisha      67.0         164   \n19682  Maize       1998  Kharif                Nagaland   30000.0       37000   \n19686  Maize       1997  Kharif       Jammu and Kashmir  310883.0      440900   \n\n       Annual_Rainfall   Fertilizer  Pesticide     Yield  mean_rainfall  \n9               2051.4   1828786.72    5956.96  0.615652          171.0  \n45              1266.7  47851190.49  155867.07  2.687778          106.0  \n46              1266.7   4648483.48   15141.64  1.980000          106.0  \n47              1266.7    926004.10    3016.30  2.165714          106.0  \n72              3818.2   1634544.75    5324.25  1.444286          319.0  \n...                ...          ...        ...       ...            ...  \n19658           1635.9   7968237.20   17194.10  1.665652          137.0  \n19659           1635.9    463081.00     999.25  2.334615          137.0  \n19660           1635.9     10867.40      23.45  2.231000          137.0  \n19682           1498.0   2964000.00    8700.00  1.225714          125.0  \n19686           1356.2  29586735.11   96373.73  1.285000          114.0  \n\n[975 rows x 11 columns]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"merged_df = pd.merge(soil_filtered_df, crop_filtered_df, left_on=['rainfall'], right_on=['mean_rainfall'])\nprint(merged_df)","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:31:32.160093Z","iopub.execute_input":"2025-04-30T14:31:32.160595Z","iopub.status.idle":"2025-04-30T14:31:32.190813Z","shell.execute_reply.started":"2025-04-30T14:31:32.160561Z","shell.execute_reply":"2025-04-30T14:31:32.189733Z"},"trusted":true},"outputs":[{"name":"stdout","text":"      N   P   K  temperature   humidity        ph  rainfall  label   Crop  \\\n0    71  54  16    22.613600  63.690706  5.749914      88.0  maize  Maize   \n1    71  54  16    22.613600  63.690706  5.749914      88.0  maize  Maize   \n2    71  54  16    22.613600  63.690706  5.749914      88.0  maize  Maize   \n3    71  54  16    22.613600  63.690706  5.749914      88.0  maize  Maize   \n4    71  54  16    22.613600  63.690706  5.749914      88.0  maize  Maize   \n..   ..  ..  ..          ...        ...       ...       ...    ...    ...   \n859  83  57  19    25.730444  70.747393  6.877869      99.0  maize  Maize   \n860  83  57  19    25.730444  70.747393  6.877869      99.0  maize  Maize   \n861  83  57  19    25.730444  70.747393  6.877869      99.0  maize  Maize   \n862  83  57  19    25.730444  70.747393  6.877869      99.0  maize  Maize   \n863  83  57  19    25.730444  70.747393  6.877869      99.0  maize  Maize   \n\n     Crop_Year       Season             State      Area  Production  \\\n0         1998  Kharif         Andhra Pradesh  314900.0     1019800   \n1         1998  Rabi           Andhra Pradesh   83700.0      363700   \n2         2004  Kharif            Maharashtra  353200.0      606900   \n3         2004  Rabi              Maharashtra   65000.0      130500   \n4         2004  Summer            Maharashtra   13400.0       20400   \n..         ...          ...               ...       ...         ...   \n859       2006  Kharif            Uttarakhand   27163.0       35850   \n860       2006  Rabi              Uttarakhand      81.0         131   \n861       2012  Kharif                Manipur   12640.0       25470   \n862       2012  Rabi                  Manipur    6800.0       19000   \n863       2017  Kharif       Himachal Pradesh  280811.0      711114   \n\n     Annual_Rainfall   Fertilizer  Pesticide     Yield  mean_rainfall  \n0             1048.3  31112120.00   91321.00  2.479545           88.0  \n1             1048.3   8269560.00   24273.00  4.245000           88.0  \n2             1052.0  38265688.00   74172.00  1.387778           88.0  \n3             1052.0   7042100.00   13650.00  1.723478           88.0  \n4             1052.0   1451756.00    2814.00  1.462778           88.0  \n..               ...          ...        ...       ...            ...  \n859           1178.9   3468986.73    5975.86  1.138462           99.0  \n860           1178.9     10344.51      17.82  1.626667           99.0  \n861           1176.7   1906112.00    3918.40  1.998889           99.0  \n862           1176.7   1025440.00    2108.00  2.796667           99.0  \n863           1182.2  44210883.84  106708.18  2.886667           99.0  \n\n[864 rows x 19 columns]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"merged_df.Crop_Year.unique()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:31:36.681784Z","iopub.execute_input":"2025-04-30T14:31:36.682158Z","iopub.status.idle":"2025-04-30T14:31:36.689902Z","shell.execute_reply.started":"2025-04-30T14:31:36.68213Z","shell.execute_reply":"2025-04-30T14:31:36.688683Z"},"trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array([1998, 2004, 2006, 2008, 2012, 2016, 2018, 2013, 2009, 1997, 2002,\n       2017, 2000, 2015, 2005, 2010, 2020, 2019, 2011, 2014, 2003, 2007,\n       1999, 2001])"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"merged_df=merged_df.drop(columns= ['label', 'Crop','mean_rainfall'])","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:31:51.550792Z","iopub.execute_input":"2025-04-30T14:31:51.551145Z","iopub.status.idle":"2025-04-30T14:31:51.559534Z","shell.execute_reply.started":"2025-04-30T14:31:51.551118Z","shell.execute_reply":"2025-04-30T14:31:51.558163Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"merged_df.to_csv('merged_df.csv',index=False)\nmerged_df.head()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:32:03.777778Z","iopub.execute_input":"2025-04-30T14:32:03.778156Z","iopub.status.idle":"2025-04-30T14:32:03.811841Z","shell.execute_reply.started":"2025-04-30T14:32:03.778125Z","shell.execute_reply":"2025-04-30T14:32:03.810644Z"},"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"    N   P   K  temperature   humidity        ph  rainfall  Crop_Year  \\\n0  71  54  16      22.6136  63.690706  5.749914      88.0       1998   \n1  71  54  16      22.6136  63.690706  5.749914      88.0       1998   \n2  71  54  16      22.6136  63.690706  5.749914      88.0       2004   \n3  71  54  16      22.6136  63.690706  5.749914      88.0       2004   \n4  71  54  16      22.6136  63.690706  5.749914      88.0       2004   \n\n        Season           State      Area  Production  Annual_Rainfall  \\\n0  Kharif       Andhra Pradesh  314900.0     1019800           1048.3   \n1  Rabi         Andhra Pradesh   83700.0      363700           1048.3   \n2  Kharif          Maharashtra  353200.0      606900           1052.0   \n3  Rabi            Maharashtra   65000.0      130500           1052.0   \n4  Summer          Maharashtra   13400.0       20400           1052.0   \n\n   Fertilizer  Pesticide     Yield  \n0  31112120.0    91321.0  2.479545  \n1   8269560.0    24273.0  4.245000  \n2  38265688.0    74172.0  1.387778  \n3   7042100.0    13650.0  1.723478  \n4   1451756.0     2814.0  1.462778  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N</th>\n      <th>P</th>\n      <th>K</th>\n      <th>temperature</th>\n      <th>humidity</th>\n      <th>ph</th>\n      <th>rainfall</th>\n      <th>Crop_Year</th>\n      <th>Season</th>\n      <th>State</th>\n      <th>Area</th>\n      <th>Production</th>\n      <th>Annual_Rainfall</th>\n      <th>Fertilizer</th>\n      <th>Pesticide</th>\n      <th>Yield</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>71</td>\n      <td>54</td>\n      <td>16</td>\n      <td>22.6136</td>\n      <td>63.690706</td>\n      <td>5.749914</td>\n      <td>88.0</td>\n      <td>1998</td>\n      <td>Kharif</td>\n      <td>Andhra Pradesh</td>\n      <td>314900.0</td>\n      <td>1019800</td>\n      <td>1048.3</td>\n      <td>31112120.0</td>\n      <td>91321.0</td>\n      <td>2.479545</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>71</td>\n      <td>54</td>\n      <td>16</td>\n      <td>22.6136</td>\n      <td>63.690706</td>\n      <td>5.749914</td>\n      <td>88.0</td>\n      <td>1998</td>\n      <td>Rabi</td>\n      <td>Andhra Pradesh</td>\n      <td>83700.0</td>\n      <td>363700</td>\n      <td>1048.3</td>\n      <td>8269560.0</td>\n      <td>24273.0</td>\n      <td>4.245000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>71</td>\n      <td>54</td>\n      <td>16</td>\n      <td>22.6136</td>\n      <td>63.690706</td>\n      <td>5.749914</td>\n      <td>88.0</td>\n      <td>2004</td>\n      <td>Kharif</td>\n      <td>Maharashtra</td>\n      <td>353200.0</td>\n      <td>606900</td>\n      <td>1052.0</td>\n      <td>38265688.0</td>\n      <td>74172.0</td>\n      <td>1.387778</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>71</td>\n      <td>54</td>\n      <td>16</td>\n      <td>22.6136</td>\n      <td>63.690706</td>\n      <td>5.749914</td>\n      <td>88.0</td>\n      <td>2004</td>\n      <td>Rabi</td>\n      <td>Maharashtra</td>\n      <td>65000.0</td>\n      <td>130500</td>\n      <td>1052.0</td>\n      <td>7042100.0</td>\n      <td>13650.0</td>\n      <td>1.723478</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>71</td>\n      <td>54</td>\n      <td>16</td>\n      <td>22.6136</td>\n      <td>63.690706</td>\n      <td>5.749914</td>\n      <td>88.0</td>\n      <td>2004</td>\n      <td>Summer</td>\n      <td>Maharashtra</td>\n      <td>13400.0</td>\n      <td>20400</td>\n      <td>1052.0</td>\n      <td>1451756.0</td>\n      <td>2814.0</td>\n      <td>1.462778</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"merged_df.info()","metadata":{"execution":{"iopub.status.busy":"2025-04-30T14:32:14.907794Z","iopub.execute_input":"2025-04-30T14:32:14.908194Z","iopub.status.idle":"2025-04-30T14:32:14.926497Z","shell.execute_reply.started":"2025-04-30T14:32:14.908164Z","shell.execute_reply":"2025-04-30T14:32:14.925031Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 864 entries, 0 to 863\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   N                864 non-null    int64  \n 1   P                864 non-null    int64  \n 2   K                864 non-null    int64  \n 3   temperature      864 non-null    float64\n 4   humidity         864 non-null    float64\n 5   ph               864 non-null    float64\n 6   rainfall         864 non-null    float64\n 7   Crop_Year        864 non-null    int64  \n 8   Season           864 non-null    object \n 9   State            864 non-null    object \n 10  Area             864 non-null    float64\n 11  Production       864 non-null    int64  \n 12  Annual_Rainfall  864 non-null    float64\n 13  Fertilizer       864 non-null    float64\n 14  Pesticide        864 non-null    float64\n 15  Yield            864 non-null    float64\ndtypes: float64(9), int64(5), object(2)\nmemory usage: 108.1+ KB\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"merged_df.sample(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T14:38:54.474109Z","iopub.execute_input":"2025-04-30T14:38:54.474531Z","iopub.status.idle":"2025-04-30T14:38:54.493932Z","shell.execute_reply.started":"2025-04-30T14:38:54.474498Z","shell.execute_reply":"2025-04-30T14:38:54.492601Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"      N   P   K  temperature   humidity        ph  rainfall  Crop_Year  \\\n466  66  44  20    19.078147  69.022986  6.740001      81.0       2012   \n438  95  38  22    19.849394  61.245001  5.730617     101.0       2016   \n50   76  44  17    20.416831  62.554248  5.855442      66.0       2004   \n558  75  53  18    20.688999  59.437534  6.864794     104.0       2007   \n607  99  39  18    19.201294  68.305790  6.112751      88.0       2008   \n\n          Season           State      Area  Production  Annual_Rainfall  \\\n466  Kharif       Andhra Pradesh  565000.0     2341000            968.7   \n438  Rabi                  Bihar  284883.0     2131513           1205.6   \n50   Rabi         Andhra Pradesh  151474.0      824920            781.8   \n558  Kharif         Chhattisgarh  100591.0      157130           1246.2   \n607  Summer          Maharashtra    9500.0       14500           1052.8   \n\n      Fertilizer  Pesticide     Yield  \n466  85202000.00  175150.00  4.040952  \n438  43658319.75   99709.05  6.058182  \n50   16410693.16   31809.54  5.235714  \n558  13418839.40   16094.56  1.468333  \n607   1358880.00     855.00  1.381000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N</th>\n      <th>P</th>\n      <th>K</th>\n      <th>temperature</th>\n      <th>humidity</th>\n      <th>ph</th>\n      <th>rainfall</th>\n      <th>Crop_Year</th>\n      <th>Season</th>\n      <th>State</th>\n      <th>Area</th>\n      <th>Production</th>\n      <th>Annual_Rainfall</th>\n      <th>Fertilizer</th>\n      <th>Pesticide</th>\n      <th>Yield</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>466</th>\n      <td>66</td>\n      <td>44</td>\n      <td>20</td>\n      <td>19.078147</td>\n      <td>69.022986</td>\n      <td>6.740001</td>\n      <td>81.0</td>\n      <td>2012</td>\n      <td>Kharif</td>\n      <td>Andhra Pradesh</td>\n      <td>565000.0</td>\n      <td>2341000</td>\n      <td>968.7</td>\n      <td>85202000.00</td>\n      <td>175150.00</td>\n      <td>4.040952</td>\n    </tr>\n    <tr>\n      <th>438</th>\n      <td>95</td>\n      <td>38</td>\n      <td>22</td>\n      <td>19.849394</td>\n      <td>61.245001</td>\n      <td>5.730617</td>\n      <td>101.0</td>\n      <td>2016</td>\n      <td>Rabi</td>\n      <td>Bihar</td>\n      <td>284883.0</td>\n      <td>2131513</td>\n      <td>1205.6</td>\n      <td>43658319.75</td>\n      <td>99709.05</td>\n      <td>6.058182</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>76</td>\n      <td>44</td>\n      <td>17</td>\n      <td>20.416831</td>\n      <td>62.554248</td>\n      <td>5.855442</td>\n      <td>66.0</td>\n      <td>2004</td>\n      <td>Rabi</td>\n      <td>Andhra Pradesh</td>\n      <td>151474.0</td>\n      <td>824920</td>\n      <td>781.8</td>\n      <td>16410693.16</td>\n      <td>31809.54</td>\n      <td>5.235714</td>\n    </tr>\n    <tr>\n      <th>558</th>\n      <td>75</td>\n      <td>53</td>\n      <td>18</td>\n      <td>20.688999</td>\n      <td>59.437534</td>\n      <td>6.864794</td>\n      <td>104.0</td>\n      <td>2007</td>\n      <td>Kharif</td>\n      <td>Chhattisgarh</td>\n      <td>100591.0</td>\n      <td>157130</td>\n      <td>1246.2</td>\n      <td>13418839.40</td>\n      <td>16094.56</td>\n      <td>1.468333</td>\n    </tr>\n    <tr>\n      <th>607</th>\n      <td>99</td>\n      <td>39</td>\n      <td>18</td>\n      <td>19.201294</td>\n      <td>68.305790</td>\n      <td>6.112751</td>\n      <td>88.0</td>\n      <td>2008</td>\n      <td>Summer</td>\n      <td>Maharashtra</td>\n      <td>9500.0</td>\n      <td>14500</td>\n      <td>1052.8</td>\n      <td>1358880.00</td>\n      <td>855.00</td>\n      <td>1.381000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"pip install pandas seaborn matplotlib","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LABEL ENCODING NON-NUMERIC COLUMNS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Assuming merged_df is your DataFrame\n# Identify non-numeric columns\nnon_numeric_columns = merged_df.select_dtypes(include=['object']).columns\n\n# Initialize the label encoder\nlabel_encoders = {}\nfor column in non_numeric_columns:\n    label_encoders[column] = LabelEncoder()\n    merged_df[column] = label_encoders[column].fit_transform(merged_df[column])\n\n# Display the first few rows of the updated DataFrame\nprint(merged_df.head(15))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:19:28.129952Z","iopub.execute_input":"2024-10-08T06:19:28.130389Z","iopub.status.idle":"2024-10-08T06:19:28.695891Z","shell.execute_reply.started":"2024-10-08T06:19:28.130348Z","shell.execute_reply":"2024-10-08T06:19:28.694566Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df_1=merged_df.drop(columns=['Production'])\nmerged_df_1.to_csv('merged_df_1.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:19:28.697213Z","iopub.execute_input":"2024-10-08T06:19:28.697612Z","iopub.status.idle":"2024-10-08T06:19:28.723934Z","shell.execute_reply.started":"2024-10-08T06:19:28.697578Z","shell.execute_reply":"2024-10-08T06:19:28.722703Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Assuming merged_df is your DataFrame\n# Identify non-numeric columns\nnon_numeric_columns = merged_df_1.select_dtypes(include=['object']).columns\n\n# Initialize the label encoder\nlabel_encoders = {}\nfor column in non_numeric_columns:\n    label_encoders[column] = LabelEncoder()\n    merged_df_1[column] = label_encoders[column].fit_transform(merged_df_1[column])\n\n# Display the first few rows of the updated DataFrame\nprint(merged_df_1.head(15))","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:19:46.263252Z","iopub.execute_input":"2024-10-08T06:19:46.263696Z","iopub.status.idle":"2024-10-08T06:19:46.28398Z","shell.execute_reply.started":"2024-10-08T06:19:46.263662Z","shell.execute_reply":"2024-10-08T06:19:46.282544Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GENERATING HEATMAP","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(42,42))\nsns.heatmap(merged_df.corr(), annot=True)\nplt.savefig('/kaggle/working/correlation_heatmap.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:19:49.254357Z","iopub.execute_input":"2024-10-08T06:19:49.254837Z","iopub.status.idle":"2024-10-08T06:19:53.214141Z","shell.execute_reply.started":"2024-10-08T06:19:49.2548Z","shell.execute_reply":"2024-10-08T06:19:53.212879Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(42,42))\nsns.heatmap(merged_df_1.corr(), annot=True)\nplt.savefig('/kaggle/working/correlation_heatmap_wo_prod.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:19:53.216109Z","iopub.execute_input":"2024-10-08T06:19:53.216497Z","iopub.status.idle":"2024-10-08T06:19:56.574989Z","shell.execute_reply.started":"2024-10-08T06:19:53.216465Z","shell.execute_reply":"2024-10-08T06:19:56.573751Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RANDOM FOREST","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# Display the first few rows of the dataset\nprint(data.head())\n\n# Separate features and target variable\nX = data.drop(columns=['Yield'])\ny = data['Yield']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year','Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the training and testing data\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:19:59.471689Z","iopub.execute_input":"2024-10-08T06:19:59.472784Z","iopub.status.idle":"2024-10-08T06:19:59.85399Z","shell.execute_reply.started":"2024-10-08T06:19:59.472727Z","shell.execute_reply":"2024-10-08T06:19:59.852649Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Initialize the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:20:01.045395Z","iopub.execute_input":"2024-10-08T06:20:01.046276Z","iopub.status.idle":"2024-10-08T06:20:02.795006Z","shell.execute_reply.started":"2024-10-08T06:20:01.046236Z","shell.execute_reply":"2024-10-08T06:20:02.793815Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PROPHET","metadata":{}},{"cell_type":"code","source":"# pip install prophet","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:08.686351Z","iopub.execute_input":"2024-09-09T14:43:08.686656Z","iopub.status.idle":"2024-09-09T14:43:08.691112Z","shell.execute_reply.started":"2024-09-09T14:43:08.686632Z","shell.execute_reply":"2024-09-09T14:43:08.690113Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# from prophet import Prophet\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# import numpy as np\n\n# # Load the dataset\n# data = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# # Drop the 'Crop' column since it has only one type of crop\n# # data = data.drop(columns=['Crop'])\n\n# # Strip whitespace from 'Season' values\n# data['Season'] = data['Season'].str.strip()\n\n# # Convert 'Crop_Year' and 'Season' into a datetime format\n# # Map seasons to months (assuming a mapping)\n# season_to_month = {\n#     'Spring': '03-01',\n#     'Summer': '06-01',\n#     'Autumn': '09-01',\n#     'Winter': '12-01',\n#     'Kharif': '07-01',  # Example for Kharif season\n#     'Rabi': '01-01'    # Example for Rabi season\n# }\n\n# # Handle missing or unexpected seasons\n# def get_season_month(season):\n#     return season_to_month.get(season, '01-01')\n\n# data['Date'] = data.apply(lambda row: f\"{row['Crop_Year']}-{get_season_month(row['Season'])}\", axis=1)\n# data['Date'] = pd.to_datetime(data['Date'])\n\n# # Prepare the dataframe for Prophet\n# df = data.rename(columns={'Date': 'ds', 'Yield': 'y'})\n# df = df[['ds', 'y', 'Annual_Rainfall', 'Fertilizer', 'Pesticide', 'rainfall']]\n\n# # Initialize the Prophet model\n# model = Prophet()\n\n# # Add additional regressors\n# model.add_regressor('Annual_Rainfall')\n# model.add_regressor('Fertilizer')\n# model.add_regressor('Pesticide')\n# # model.add_regressor('Production')\n# model.add_regressor('rainfall')\n\n# # Fit the model\n# model.fit(df)\n\n# # Make future dataframe for predictions\n# future = model.make_future_dataframe(periods=12, freq='M')\n# # Fill the future dataframe with the mean of the regressors for now\n# future['Annual_Rainfall'] = df['Annual_Rainfall'].mean()\n# future['Fertilizer'] = df['Fertilizer'].mean()\n# future['Pesticide'] = df['Pesticide'].mean()\n# # future['Production'] = df['Production'].mean()\n# future['rainfall'] = df['rainfall'].mean()\n\n# # Make predictions\n# forecast = model.predict(future)\n\n# # Evaluate the model on the historical data\n# df_forecast = forecast[['ds', 'yhat']].merge(df, on='ds')\n# mae = mean_absolute_error(df_forecast['y'], df_forecast['yhat'])\n# mse = mean_squared_error(df_forecast['y'], df_forecast['yhat'])\n# rmse = np.sqrt(mse)\n# r2 = r2_score(df_forecast['y'], df_forecast['yhat'])\n\n# print(f'MAE: {mae}')\n# print(f'MSE: {mse}')\n# print(f'RMSE: {rmse}')\n# print(f'R2: {r2}')\n\n# # Plot the forecast\n# model.plot(forecast)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:08.692392Z","iopub.execute_input":"2024-09-09T14:43:08.692716Z","iopub.status.idle":"2024-09-09T14:43:08.702529Z","shell.execute_reply.started":"2024-09-09T14:43:08.69269Z","shell.execute_reply":"2024-09-09T14:43:08.701492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Back Propagation Neural Network (BPNN)","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense\n# import numpy as np\n\n# # Load the dataset\n# data = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# # Drop the 'Crop' column since it has only one type of crop\n# # data = data.drop(columns=['Crop'])\n\n# # Separate features and target variable\n# X = data.drop(columns=['Yield'])\n# y = data['Yield']\n\n# # Identify categorical and numerical columns\n# categorical_cols = ['Crop_Year', 'Season', 'State']\n# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# # Preprocessing for numerical data: impute missing values and scale features\n# numerical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean')),\n#     ('scaler', StandardScaler())\n# ])\n\n# # Preprocessing for categorical data: impute missing values and one-hot encode\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# # Combine preprocessing steps\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_transformer, numerical_cols),\n#         ('cat', categorical_transformer, categorical_cols)\n#     ])\n\n# # Apply preprocessing to the features\n# X_preprocessed = preprocessor.fit_transform(X)\n\n# # Convert sparse matrix to dense\n# X_preprocessed = X_preprocessed.toarray()\n\n# # Split the dataset into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# # Build the Neural Network\n# model = Sequential()\n# model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n# model.add(Dense(64, activation='relu'))\n# model.add(Dense(32, activation='relu'))\n# model.add(Dense(1))\n\n# # Compile the model\n# model.compile(optimizer='adam', loss='mean_squared_error')\n\n# # Train the model\n# model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n\n# # Make predictions on the test set\n# y_pred = model.predict(X_test)\n\n# # Evaluate the model\n# mae = mean_absolute_error(y_test, y_pred)\n# mse = mean_squared_error(y_test, y_pred)\n# rmse = np.sqrt(mse)\n# r2 = r2_score(y_test, y_pred)\n\n# print(f'MAE: {mae}')\n# print(f'MSE: {mse}')\n# print(f'RMSE: {rmse}')\n# print(f'R2: {r2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:08.703854Z","iopub.execute_input":"2024-09-09T14:43:08.704209Z","iopub.status.idle":"2024-09-09T14:43:08.72007Z","shell.execute_reply.started":"2024-09-09T14:43:08.704155Z","shell.execute_reply":"2024-09-09T14:43:08.718941Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Support Vector Regression (SVR)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# Drop the 'Crop' column since it has only one type of crop\n# data = data.drop(columns=['Crop'])\n\n# Separate features and target variable\nX = data.drop(columns=['Yield'])\ny = data['Yield']\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year', 'Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the features\nX_preprocessed = preprocessor.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# Train the SVR model\nsvr = SVR(kernel='rbf')\nsvr.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = svr.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'MAE: {mae}')\nprint(f'MSE: {mse}')\nprint(f'RMSE: {rmse}')\nprint(f'R2: {r2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:20:08.97093Z","iopub.execute_input":"2024-10-08T06:20:08.97139Z","iopub.status.idle":"2024-10-08T06:20:09.068258Z","shell.execute_reply.started":"2024-10-08T06:20:08.971355Z","shell.execute_reply":"2024-10-08T06:20:09.066785Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gaussian Process Regression (GPR)","metadata":{}},{"cell_type":"code","source":"# # gpr on yield\n\n# import numpy as np\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# from sklearn.gaussian_process import GaussianProcessRegressor\n# from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\n# # Load the dataset\n# data = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# # Drop the 'Crop' column since it has only one type of crop\n# # data = data.drop(columns=['Crop'])\n\n# # Separate features and target variable\n# X = data.drop(columns=['Yield'])\n# y = data['Yield']\n\n# # Identify categorical and numerical columns\n# categorical_cols = ['Crop_Year', 'Season', 'State']\n# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# # Preprocessing for numerical data: impute missing values and scale features\n# numerical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean')),\n#     ('scaler', StandardScaler())\n# ])\n\n# # Preprocessing for categorical data: impute missing values and one-hot encode\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# # Combine preprocessing steps\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_transformer, numerical_cols),\n#         ('cat', categorical_transformer, categorical_cols)\n#     ])\n\n# # Apply preprocessing to the features\n# X_preprocessed = preprocessor.fit_transform(X)\n\n# # Convert the sparse matrix to a dense format\n# X_preprocessed = X_preprocessed.toarray()\n\n# # Split the dataset into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# # Define the kernel for GPR\n# kernel = C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))\n\n# # Create and fit the GPR model\n# gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n# gpr.fit(X_train, y_train)\n\n# # Make predictions on the test set\n# y_pred, y_std = gpr.predict(X_test, return_std=True)\n\n# # Evaluate the model\n# mae = mean_absolute_error(y_test, y_pred)\n# mse = mean_squared_error(y_test, y_pred)\n# rmse = np.sqrt(mse)\n# r2 = r2_score(y_test, y_pred)\n\n# print(f'MAE: {mae}')\n# print(f'MSE: {mse}')\n# print(f'RMSE: {rmse}')\n# print(f'R2: {r2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:20:10.67704Z","iopub.execute_input":"2024-10-08T06:20:10.67743Z","iopub.status.idle":"2024-10-08T06:20:10.685883Z","shell.execute_reply.started":"2024-10-08T06:20:10.677399Z","shell.execute_reply":"2024-10-08T06:20:10.684423Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # gpr on production\n\n# import numpy as np\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# from sklearn.gaussian_process import GaussianProcessRegressor\n# from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\n# # Load the dataset\n# data = pd.read_csv('/kaggle/working/merged_df.csv')\n\n# # Drop the 'Crop' column since it has only one type of crop\n# # data = data.drop(columns=['Crop'])\n\n# # Separate features and target variable\n# X = data.drop(columns=['Production'])\n# y = data['Production']\n\n# # Identify categorical and numerical columns\n# categorical_cols = ['Crop_Year', 'Season', 'State']\n# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# # Preprocessing for numerical data: impute missing values and scale features\n# numerical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean')),\n#     ('scaler', StandardScaler())\n# ])\n\n# # Preprocessing for categorical data: impute missing values and one-hot encode\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# # Combine preprocessing steps\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_transformer, numerical_cols),\n#         ('cat', categorical_transformer, categorical_cols)\n#     ])\n\n# # Apply preprocessing to the features\n# X_preprocessed = preprocessor.fit_transform(X)\n\n# # Convert the sparse matrix to a dense format\n# X_preprocessed = X_preprocessed.toarray()\n\n# # Split the dataset into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# # Define the kernel for GPR\n# kernel = C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))\n\n# # Create and fit the GPR model\n# gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n# gpr.fit(X_train, y_train)\n\n# # Make predictions on the test set\n# y_pred, y_std = gpr.predict(X_test, return_std=True)\n\n# # Evaluate the model\n# mae = mean_absolute_error(y_test, y_pred)\n# mse = mean_squared_error(y_test, y_pred)\n# rmse = np.sqrt(mse)\n# r2 = r2_score(y_test, y_pred)\n\n# print(f'MAE: {mae}')\n# print(f'MSE: {mse}')\n# print(f'RMSE: {rmse}')\n# print(f'R2: {r2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:08.827951Z","iopub.execute_input":"2024-09-09T14:43:08.828399Z","iopub.status.idle":"2024-09-09T14:43:08.844302Z","shell.execute_reply.started":"2024-09-09T14:43:08.828369Z","shell.execute_reply":"2024-09-09T14:43:08.843122Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"pip install xgboost","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:20:14.846463Z","iopub.execute_input":"2024-10-08T06:20:14.846901Z","iopub.status.idle":"2024-10-08T06:20:50.19362Z","shell.execute_reply.started":"2024-10-08T06:20:14.846866Z","shell.execute_reply":"2024-10-08T06:20:50.192183Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom xgboost import XGBRegressor\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# Separate features and target variable\nX = data.drop(columns=['Yield'])\ny = data['Yield']\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year', 'Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the features\nX_preprocessed = preprocessor.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# Initialize the XGBoost Regressor\nxgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n\n# Train the model\nxgb_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = xgb_model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:20:50.196323Z","iopub.execute_input":"2024-10-08T06:20:50.196751Z","iopub.status.idle":"2024-10-08T06:20:50.607605Z","shell.execute_reply.started":"2024-10-08T06:20:50.196711Z","shell.execute_reply":"2024-10-08T06:20:50.606564Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"# pip install lightgbm","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:41.300434Z","iopub.execute_input":"2024-09-09T14:43:41.301104Z","iopub.status.idle":"2024-09-09T14:43:41.305921Z","shell.execute_reply.started":"2024-09-09T14:43:41.301068Z","shell.execute_reply":"2024-09-09T14:43:41.304723Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# import lightgbm as lgb\n\n# # Load the dataset\n# data = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# # Separate features and target variable\n# X = data.drop(columns=['Yield'])\n# y = data['Yield']\n\n# # Identify categorical and numerical columns\n# categorical_cols = ['Crop_Year', 'Season', 'State']\n# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# # Preprocessing for numerical data: impute missing values and scale features\n# numerical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean')),\n#     ('scaler', StandardScaler())\n# ])\n\n# # Preprocessing for categorical data: impute missing values and one-hot encode\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# # Combine preprocessing steps\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_transformer, numerical_cols),\n#         ('cat', categorical_transformer, categorical_cols)\n#     ])\n\n# # Apply preprocessing to the features\n# X_preprocessed = preprocessor.fit_transform(X)\n\n# # Split the dataset into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# # Initialize the LightGBM Regressor\n# lgb_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n\n# # Train the model\n# lgb_model.fit(X_train, y_train)\n\n# # Make predictions on the test set\n# y_pred = lgb_model.predict(X_test)\n\n# # Evaluate the model\n# mae = mean_absolute_error(y_test, y_pred)\n# mse = mean_squared_error(y_test, y_pred)\n# rmse = np.sqrt(mse)\n# r2 = r2_score(y_test, y_pred)\n\n# print(f'Mean Absolute Error: {mae}')\n# print(f'Mean Squared Error: {mse}')\n# print(f'Root Mean Squared Error: {rmse}')\n# print(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:43:41.307457Z","iopub.execute_input":"2024-09-09T14:43:41.307786Z","iopub.status.idle":"2024-09-09T14:43:41.319618Z","shell.execute_reply.started":"2024-09-09T14:43:41.307731Z","shell.execute_reply":"2024-09-09T14:43:41.318455Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CATBoost","metadata":{}},{"cell_type":"code","source":"pip install catboost","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:20:59.735856Z","iopub.execute_input":"2024-10-08T06:20:59.736748Z","iopub.status.idle":"2024-10-08T06:21:35.113867Z","shell.execute_reply.started":"2024-10-08T06:20:59.736707Z","shell.execute_reply":"2024-10-08T06:21:35.112343Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom catboost import CatBoostRegressor\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# Separate features and target variable\nX = data.drop(columns=['Yield'])\ny = data['Yield']\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year', 'Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the features\nX_preprocessed = preprocessor.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# Initialize the CatBoost Regressor\ncatboost_model = CatBoostRegressor(iterations=100, learning_rate=0.1, random_state=42, verbose=0)\n\n# Train the model\ncatboost_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = catboost_model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:21:35.116524Z","iopub.execute_input":"2024-10-08T06:21:35.116936Z","iopub.status.idle":"2024-10-08T06:21:35.847511Z","shell.execute_reply.started":"2024-10-08T06:21:35.116899Z","shell.execute_reply":"2024-10-08T06:21:35.846206Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RNN","metadata":{}},{"cell_type":"code","source":"# pip install tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:44:14.140318Z","iopub.execute_input":"2024-09-09T14:44:14.140776Z","iopub.status.idle":"2024-09-09T14:44:14.145537Z","shell.execute_reply.started":"2024-09-09T14:44:14.14074Z","shell.execute_reply":"2024-09-09T14:44:14.144462Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, LSTM, Dropout\n\n# # Load the dataset\n# data = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# # Separate features and target variable\n# X = data.drop(columns=['Yield'])\n# y = data['Yield']\n\n# # Identify categorical and numerical columns\n# categorical_cols = ['Crop_Year', 'Season', 'State']\n# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# # Preprocessing for numerical data: impute missing values and scale features\n# numerical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean')),\n#     ('scaler', StandardScaler())\n# ])\n\n# # Preprocessing for categorical data: impute missing values and one-hot encode\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n# ])\n\n# # Combine preprocessing steps\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_transformer, numerical_cols),\n#         ('cat', categorical_transformer, categorical_cols)\n#     ])\n\n# # Apply preprocessing to the features\n# X_preprocessed = preprocessor.fit_transform(X)\n\n# # Convert the sparse matrix to a dense one\n# X_preprocessed = X_preprocessed.toarray()\n\n# # Reshape the data to fit the LSTM input format (samples, time steps, features)\n# # Here we assume each sample is one time step\n# X_preprocessed = X_preprocessed.reshape((X_preprocessed.shape[0], 1, X_preprocessed.shape[1]))\n\n# # Split the dataset into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# # Define the RNN model\n# model = Sequential()\n# model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n# model.add(Dropout(0.2))\n# model.add(Dense(1))\n# model.compile(optimizer='adam', loss='mean_squared_error')\n\n# # Train the model\n# model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n\n# # Make predictions on the test set\n# y_pred = model.predict(X_test)\n\n# # Evaluate the model\n# mae = mean_absolute_error(y_test, y_pred)\n# mse = mean_squared_error(y_test, y_pred)\n# rmse = np.sqrt(mse)\n# r2 = r2_score(y_test, y_pred)\n\n# print(f'Mean Absolute Error: {mae}')\n# print(f'Mean Squared Error: {mse}')\n# print(f'Root Mean Squared Error: {rmse}')\n# print(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:44:14.147113Z","iopub.execute_input":"2024-09-09T14:44:14.147637Z","iopub.status.idle":"2024-09-09T14:44:14.166916Z","shell.execute_reply.started":"2024-09-09T14:44:14.1476Z","shell.execute_reply":"2024-09-09T14:44:14.165866Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next things to try:\n1. drop yield and get results with production as output (not needed)\n2. drop production and get results with yield as output (done)\n3. apply random forest on a set of features to analyse most imp combo of features","metadata":{}},{"cell_type":"code","source":"# cross validation on random forest \n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/working/merged_df_1.csv')\n\n# Drop the 'Crop' column since it has only one type of crop\n# data = data.drop(columns=['Crop'])\n\n# Separate features and target variable\nX = data.drop(columns=['Yield'])\ny = data['Yield']\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year', 'Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the features\nX_preprocessed = preprocessor.fit_transform(X)\n\n# Define the cross-validation strategy\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Initialize the Random Forest Regressor\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Define scoring metrics\nscoring = {\n    'MAE': make_scorer(mean_absolute_error),\n    'MSE': make_scorer(mean_squared_error),\n    'R2': make_scorer(r2_score)\n}\n\n# Perform cross-validation and get the scores\ncv_results = cross_validate(rf_model, X_preprocessed, y, cv=kf, scoring=scoring, return_train_score=True)\n\n# Display cross-validation results\nprint(\"Cross-Validation Results:\")\nprint(f\"Mean Absolute Error: {np.mean(cv_results['test_MAE'])}\")\nprint(f\"Mean Squared Error: {np.mean(cv_results['test_MSE'])}\")\nprint(f\"R2 Score: {np.mean(cv_results['test_R2'])}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:21:57.696231Z","iopub.execute_input":"2024-10-08T06:21:57.69767Z","iopub.status.idle":"2024-10-08T06:22:05.593105Z","shell.execute_reply.started":"2024-10-08T06:21:57.697625Z","shell.execute_reply":"2024-10-08T06:22:05.591962Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df_1.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:05.594802Z","iopub.execute_input":"2024-10-08T06:22:05.595132Z","iopub.status.idle":"2024-10-08T06:22:05.619501Z","shell.execute_reply.started":"2024-10-08T06:22:05.595104Z","shell.execute_reply":"2024-10-08T06:22:05.618241Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df_1.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:08.958656Z","iopub.execute_input":"2024-10-08T06:22:08.959254Z","iopub.status.idle":"2024-10-08T06:22:08.973237Z","shell.execute_reply.started":"2024-10-08T06:22:08.959216Z","shell.execute_reply":"2024-10-08T06:22:08.972078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comparing different attributes with RF","metadata":{}},{"cell_type":"code","source":"# total 14 input attributes\n# ignoring the categorical features Crop_Year, Season, State\n# now have 11 options\n# making random attribute sets from these 11 to find most imp attributes\n\nset_output=merged_df_1['Yield']","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:12.070923Z","iopub.execute_input":"2024-10-08T06:22:12.071374Z","iopub.status.idle":"2024-10-08T06:22:12.077321Z","shell.execute_reply.started":"2024-10-08T06:22:12.071337Z","shell.execute_reply":"2024-10-08T06:22:12.075918Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attr_set_1=['Crop_Year','Season', 'State','temperature','ph','rainfall','Fertilizer','Pesticide']\nset_1=merged_df_1[attr_set_1].copy()\n\n# Separate features and target variable\nX = set_1\ny = set_output\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year','Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the training and testing data\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\n\n# Initialize the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:12.833061Z","iopub.execute_input":"2024-10-08T06:22:12.833524Z","iopub.status.idle":"2024-10-08T06:22:13.957799Z","shell.execute_reply.started":"2024-10-08T06:22:12.833484Z","shell.execute_reply":"2024-10-08T06:22:13.9563Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attr_set_2=['Crop_Year','Season', 'State','N','P','K','humidity']\nset_2=merged_df_1[attr_set_2].copy()\n# Separate features and target variable\nX = set_2\ny = set_output\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year','Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the training and testing data\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\n\n# Initialize the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:13.960842Z","iopub.execute_input":"2024-10-08T06:22:13.961336Z","iopub.status.idle":"2024-10-08T06:22:14.957695Z","shell.execute_reply.started":"2024-10-08T06:22:13.961294Z","shell.execute_reply":"2024-10-08T06:22:14.956381Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attr_set_3=['Crop_Year','Season', 'State', 'temperature', 'ph', 'Fertilizer', 'N','P','K']\nset_3=merged_df_1[attr_set_3].copy()\n\n# Separate features and target variable\nX = set_3\ny = set_output\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year','Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the training and testing data\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\n\n# Initialize the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:14.959531Z","iopub.execute_input":"2024-10-08T06:22:14.960028Z","iopub.status.idle":"2024-10-08T06:22:16.156957Z","shell.execute_reply.started":"2024-10-08T06:22:14.959969Z","shell.execute_reply":"2024-10-08T06:22:16.155566Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"attr_set_4=['Crop_Year','Season', 'State', 'temperature', 'ph', 'N','P','K']\nset_4=merged_df_1[attr_set_4].copy()\n\n# Separate features and target variable\nX = set_4\ny = set_output\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Identify categorical and numerical columns\ncategorical_cols = ['Crop_Year','Season', 'State']\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.difference(categorical_cols)\n\n# Preprocessing for numerical data: impute missing values and scale features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Preprocessing for categorical data: impute missing values and one-hot encode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Apply preprocessing to the training and testing data\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\n\n# Initialize the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'R2 Score: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:16.159242Z","iopub.execute_input":"2024-10-08T06:22:16.159665Z","iopub.status.idle":"2024-10-08T06:22:17.229349Z","shell.execute_reply.started":"2024-10-08T06:22:16.15963Z","shell.execute_reply":"2024-10-08T06:22:17.227701Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PLOTTING MODEL COMPARISONS","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Define the attribute sets and their corresponding R2 scores\nattr_sets = ['Attr Set 1', 'Attr Set 2', 'Attr Set 3', 'Attr Set 4']\nr2_scores = [0.9534964916495487, 0.9011542128032501, 0.9359800615312223, 0.8927537138619698]\n\n# Create a figure and axis\nplt.figure(figsize=(10, 6))\n\n# Plot the R2 scores for different attribute sets\nplt.plot(attr_sets, r2_scores, marker='o', linestyle='-', color='blue', label='R2 Score')\n\n# Annotate points with their R2 score values\nfor i, score in enumerate(r2_scores):\n    plt.text(attr_sets[i], score, f'{score:.4f}', ha='center', va='bottom')\n\n# Title and labels\nplt.title('R2 Scores for Different Attribute Sets Using Random Forest')\nplt.ylabel('R2 Score')\nplt.xlabel('Attribute Sets')\nplt.ylim(0, 1)  # Set y-axis limit for better visualization\nplt.legend()\nplt.grid(True)\n\n# Save the figure\nplt.savefig('/kaggle/working/RF_comparisons.png')\n\n# Display the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:18.023512Z","iopub.execute_input":"2024-10-08T06:22:18.024539Z","iopub.status.idle":"2024-10-08T06:22:18.489754Z","shell.execute_reply.started":"2024-10-08T06:22:18.024489Z","shell.execute_reply":"2024-10-08T06:22:18.488527Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Define the attribute sets and their corresponding R2 scores\nattr_sets = ['Attr Set 1', 'Attr Set 2', 'Attr Set 3', 'Attr Set 4']\nr2_scores = [0.9534964916495487, 0.9011542128032501, 0.9359800615312223, 0.8927537138619698]\n\n# Define hatch patterns for each bar\npatterns = ['/', '\\\\', '|', '-']\n\n# Create a smaller figure and axis\nplt.figure(figsize=(10, 6))\n\n# Plot the R2 scores as a bar chart with different patterns\nbars = plt.bar(attr_sets, r2_scores, hatch=patterns[0], color='white', edgecolor='black')\n\n# Apply different hatch patterns to each bar\nfor bar, pattern in zip(bars, patterns):\n    bar.set_hatch(pattern)\n\n# Annotate bars with their R2 score values (smaller font and close to bars)\nfor i, score in enumerate(r2_scores):\n    plt.text(i, score + 0.005, f'{score:.4f}', ha='center', va='bottom', fontsize=8)\n\n# Title and labels (smaller font size)\n\nplt.ylabel('R2 Score', fontsize=9)\nplt.xlabel('Attribute Sets', fontsize=9)\nplt.ylim(0.8, 1)  # Set y-axis limit to focus on the range 0.8 to 1\nplt.grid(False)\n\n# Save the figure\nplt.savefig('/kaggle/working/RF_comparisons_bar.png')\n\n# Display the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:57:11.85189Z","iopub.execute_input":"2024-10-22T17:57:11.852493Z","iopub.status.idle":"2024-10-22T17:57:12.265517Z","shell.execute_reply.started":"2024-10-22T17:57:11.852446Z","shell.execute_reply":"2024-10-22T17:57:12.264098Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Results from different models\nmodels = ['Random Forest', 'XGBoost', 'SVR', 'CatBoost']\nr2_scores = [0.9550701929644764, 0.9436145383374764, 0.6003614289271699, 0.1771460132644208]\n\n# Create the line plot for R2 scores\nplt.figure(figsize=(10, 6))\n\n# Plot each model's R2 score with individual lines\nplt.plot(models, r2_scores, marker='o', linestyle='-', color='b', label='R2 Score')\n\n# Highlight individual scores with markers\nfor i, score in enumerate(r2_scores):\n    plt.text(models[i], score, f'{score:.2f}', ha='center', va='bottom')\n\nplt.title('R2 Scores of Different Models')\nplt.ylabel('R2 Score')\nplt.ylim(0, 1)\nplt.xlabel('Models')\nplt.grid(axis='y')\nplt.legend()\n\n# Display the plot\nplt.savefig('/kaggle/working/r2_scores.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:22:29.120827Z","iopub.execute_input":"2024-10-08T06:22:29.121263Z","iopub.status.idle":"2024-10-08T06:22:29.578334Z","shell.execute_reply.started":"2024-10-08T06:22:29.121229Z","shell.execute_reply":"2024-10-08T06:22:29.57686Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Metrics\nmetrics = ['MAE', 'MSE', 'RMSE', 'R2 Score']\n\n# Results for each model\nresults = {\n    'Random Forest': [0.203, 0.107, 0.327, 0.955],\n    'XGBoost': [0.225, 0.134, 0.366, 0.944],\n    'SVR': [0.531, 0.952, 0.976, 0.600],\n    'CatBoost': [0.932, 1.960, 1.400, 0.177]\n}\n\n# Create a figure and axis\nplt.figure(figsize=(12, 8))\n\n# Plot each model's metrics\nfor model, values in results.items():\n    plt.plot(metrics, values, marker='o', linestyle='-', label=model)\n\n# Annotate points with their values\nfor model, values in results.items():\n    for i, value in enumerate(values):\n        plt.text(metrics[i], value, f'{value:.3f}', ha='center', va='bottom')\n\n# Title and labels\nplt.title('Model Performance Across Different Metrics')\nplt.ylabel('Metric Value')\nplt.xlabel('Metrics')\nplt.legend()\nplt.grid(True)\n\n# Display the plot\nplt.savefig('/kaggle/working/model_performances.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:26:06.531634Z","iopub.execute_input":"2024-10-08T06:26:06.532142Z","iopub.status.idle":"2024-10-08T06:26:07.174869Z","shell.execute_reply.started":"2024-10-08T06:26:06.532104Z","shell.execute_reply":"2024-10-08T06:26:07.17354Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Metrics\nmetrics = ['MAE', 'MSE', 'RMSE', 'R2 Score']\n\n# Results for each model\nresults = {\n    'Random Forest': [0.203, 0.107, 0.327, 0.955],\n    'XGBoost': [0.225, 0.134, 0.366, 0.944],\n    'SVR': [0.531, 0.952, 0.976, 0.600],\n    'CatBoost': [0.932, 1.960, 1.400, 0.177]\n}\n\n# Define the number of models and metrics\nn_models = len(results)\nn_metrics = len(metrics)\n\n# Set up bar width and index positions for each group of bars\nbar_width = 0.2\nindex = np.arange(n_metrics)\n\n# Create a figure and axis\nplt.figure(figsize=(12, 8))\n\n# Plot each model's metrics as bars\nfor i, (model, values) in enumerate(results.items()):\n    plt.bar(index + i * bar_width, values, bar_width, label=model)\n\n# Annotate bars with their values\nfor i, (model, values) in enumerate(results.items()):\n    for j, value in enumerate(values):\n        plt.text(index[j] + i * bar_width, value + 0.02, f'{value:.3f}', ha='center', va='bottom')\n\n# Title and labels\nplt.title('Model Performance Across Different Metrics')\nplt.ylabel('Metric Value')\nplt.xlabel('Metrics')\nplt.xticks(index + bar_width * (n_models - 1) / 2, metrics)\nplt.legend()\nplt.grid(True, axis='y')\n\n# Display the plot\nplt.savefig('/kaggle/working/model_performances_bar.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:30:06.376368Z","iopub.execute_input":"2024-10-08T06:30:06.3774Z","iopub.status.idle":"2024-10-08T06:30:06.999178Z","shell.execute_reply.started":"2024-10-08T06:30:06.377353Z","shell.execute_reply":"2024-10-08T06:30:06.997892Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Results for each model\nresults = {\n    'Random Forest': 0.203,\n    'XGBoost': 0.225,\n    'SVR': 0.531,\n    'CatBoost': 0.932\n}\n\n# Define consistent hatch patterns for each model\npatterns = ['/', '..', 'x', '--']\n\n# Create a bar plot for MAE\nplt.figure(figsize=(8, 5), facecolor='white')  # Set figure background to white\nbars = plt.bar(results.keys(), results.values(), edgecolor='black', hatch=patterns, color='none')\n\n# Annotate bars with their values\nfor bar, value in zip(bars, results.values()):\n    plt.text(bar.get_x() + bar.get_width() / 2, value + 0.02, f'{value:.3f}', ha='center', va='bottom')\n\n# Labels\nplt.ylabel('MAE Value')\nplt.xlabel('Models')\nplt.ylim(0, 1)  # Set y-axis limit for better visualization\n\n# Remove grid lines\nplt.grid(False)\n\n# Save and display the plot\nplt.savefig('/kaggle/working/MAE_models_bar.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:50:34.929642Z","iopub.execute_input":"2024-10-22T17:50:34.93009Z","iopub.status.idle":"2024-10-22T17:50:35.35293Z","shell.execute_reply.started":"2024-10-22T17:50:34.930056Z","shell.execute_reply":"2024-10-22T17:50:35.351431Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Results for each model\nresults = {\n    'Random Forest': 0.107,\n    'XGBoost': 0.134,\n    'SVR': 0.952,\n    'CatBoost': 1.960\n}\n\n# Define consistent hatch patterns for each model\npatterns = ['/', '..', 'x', '--']\n\n# Create a bar plot for MSE\nplt.figure(figsize=(8, 5), facecolor='white')  # Set figure background to white\nbars = plt.bar(results.keys(), results.values(), edgecolor='black', hatch=patterns, color='none')\n\n# Annotate bars with their values\nfor bar, value in zip(bars, results.values()):\n    plt.text(bar.get_x() + bar.get_width() / 2, value + 0.05, f'{value:.3f}', ha='center', va='bottom')\n\n# Title and labels\n\nplt.ylabel('MSE Value')\nplt.xlabel('Models')\nplt.ylim(0, 2.5)  # Set y-axis limit for better visualization\nplt.grid(False)\n\n# Save and display the plot\nplt.savefig('/kaggle/working/MSE_models_bar.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:51:19.109875Z","iopub.execute_input":"2024-10-22T17:51:19.110888Z","iopub.status.idle":"2024-10-22T17:51:19.464274Z","shell.execute_reply.started":"2024-10-22T17:51:19.110832Z","shell.execute_reply":"2024-10-22T17:51:19.462886Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Results for each model\nresults = {\n    'Random Forest': 0.327,\n    'XGBoost': 0.366,\n    'SVR': 0.976,\n    'CatBoost': 1.400\n}\n\n# Define consistent hatch patterns for each model\npatterns = ['/', '..', 'x', '--']\n\n# Create a bar plot for RMSE\nplt.figure(figsize=(8, 5), facecolor='white')  # Set figure background to white\nbars = plt.bar(results.keys(), results.values(), edgecolor='black', hatch=patterns, color='none')\n\n# Annotate bars with their values\nfor bar, value in zip(bars, results.values()):\n    plt.text(bar.get_x() + bar.get_width() / 2, value + 0.05, f'{value:.3f}', ha='center', va='bottom')\n\n# Title and labels\n\nplt.ylabel('RMSE Value')\nplt.xlabel('Models')\nplt.ylim(0, 1.6)  # Set y-axis limit for better visualization\nplt.grid(False)\n\n# Save and display the plot\nplt.savefig('/kaggle/working/RMSE_models_bar.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:51:27.719358Z","iopub.execute_input":"2024-10-22T17:51:27.719823Z","iopub.status.idle":"2024-10-22T17:51:28.088412Z","shell.execute_reply.started":"2024-10-22T17:51:27.719789Z","shell.execute_reply":"2024-10-22T17:51:28.087164Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Results for each model\nresults = {\n    'Random Forest': 0.955,\n    'XGBoost': 0.944,\n    'SVR': 0.600,\n    'CatBoost': 0.177\n}\n\n# Define consistent hatch patterns for each model\npatterns = ['/', '..', 'x', '--']\n\n# Create a bar plot for R Score\nplt.figure(figsize=(8, 5), facecolor='white')  # Set figure background to white\nbars = plt.bar(results.keys(), results.values(), edgecolor='black', hatch=patterns, color='none')\n\n# Annotate bars with their values\nfor bar, value in zip(bars, results.values()):\n    plt.text(bar.get_x() + bar.get_width() / 2, value + 0.02, f'{value:.3f}', ha='center', va='bottom')\n\n# Title and labels\n\nplt.ylabel('R Score Value')\nplt.xlabel('Models')\nplt.ylim(0, 1.1)  # Set y-axis limit for better visualization\nplt.grid(False)\n\n# Save and display the plot\nplt.savefig('/kaggle/working/R2_models_bar.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T17:52:41.672758Z","iopub.execute_input":"2024-10-22T17:52:41.673749Z","iopub.status.idle":"2024-10-22T17:52:42.040323Z","shell.execute_reply.started":"2024-10-22T17:52:41.673708Z","shell.execute_reply":"2024-10-22T17:52:42.039036Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom IPython.display import display, Markdown\n\n# Define the data\ndata = {\n    'Model': ['Random Forest', 'XGBoost', 'SVR', 'CatBoost'],\n    'Mean Absolute Error': [0.2031, 0.2254, 0.5311, 0.9317],\n    'Mean Squared Error': [0.1070, 0.1343, 0.9520, 1.9602],\n    'Root Mean Squared Error': [0.3272, 0.3665, 0.9757, 1.4001],\n    'R2 Score': [0.9551, 0.9436, 0.6004, 0.1771]\n}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Display the table in a readable format using markdown\ntable_md = df.to_markdown(index=False)\ndisplay(Markdown(table_md))\n\n# Save the DataFrame to a CSV file\n# df.to_csv('/kaggle/working/model_performance_table.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:44:27.796547Z","iopub.execute_input":"2024-09-09T14:44:27.799568Z","iopub.status.idle":"2024-09-09T14:44:27.865503Z","shell.execute_reply.started":"2024-09-09T14:44:27.799524Z","shell.execute_reply":"2024-09-09T14:44:27.864331Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}